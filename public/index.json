[{"content":"The Journey So Far Let\u0026rsquo;s recap this wild ride:\nThe Problem: YouTube reviewers blamed NVIDIA hardware for being slow The Investigation: I found 20-30GB memory overhead in Docker containers The Environment Setup: MPI and chroot configuration nightmare The Revelation: Docker\u0026rsquo;s cgroups double-count unified memory The Data: 60 runs confirmed the pattern consistently Now, what do I actually do with this knowledge?\nKey Finding: Don\u0026rsquo;t Blame the Hardware The most important lesson from this entire investigation:\nHardware isn\u0026rsquo;t the problem when you haven\u0026rsquo;t understood the software stack.\nThose YouTube reviews that said \u0026ldquo;DGX Spark is slow\u0026rdquo; or \u0026ldquo;Grace Hopper is disappointing\u0026rdquo;? They were wrong. Not because the numbers were wrong, but because they stopped at the numbers.\nThe hardware is fine. The software assumptions are outdated.\nDocker\u0026rsquo;s cgroups were designed in an era of discrete GPUs where:\nCPU RAM and GPU VRAM are separate Memory spaces don\u0026rsquo;t overlap No double-counting is possible Grace Hopper introduced unified memory:\nOne coherent memory pool Both processors access the same RAM Elegant\u0026hellip; but Docker doesn\u0026rsquo;t understand it yet The lesson: Dig deeper. Understand the full stack. Don\u0026rsquo;t just blame the hardware.\nPractical Recommendations Based on my findings, here\u0026rsquo;s what I recommend:\nFor Large Models on Grace Hopper (\u0026gt; 10B params): ‚úÖ Use Native/Chroot Execution\nWhy:\n20-30 GB memory savings 1.7-2.7x more KV cache No performance penalty Better resource utilization Trade-off: Less isolation, more setup complexity\nFor Small Models (\u0026lt; 10B params): ü§î Docker is Acceptable\nIf 30GB overhead is acceptable for your use case:\nEasier deployment and management Better isolation for multi-tenancy Standard container tooling Simpler CI/CD integration For Discrete GPU Systems (H100, A100): ‚ö†Ô∏è This Finding is Grace Hopper Specific\nTraditional discrete GPU systems should NOT exhibit this pattern because:\nGPU VRAM is outside Docker\u0026rsquo;s cgroups No double-counting possible Standard container best practices apply The KV Cache Deep Dive (Phase 2) Here\u0026rsquo;s what really caught my attention: The KV cache scaling.\nLook at these numbers again:\nModel Size Native KV Cache DeepSeek 7B 44.31 GiB Qwen 72B 44.71 GiB GPT-OSS 120B 43.19 GiB Wait\u0026hellip; the 72B model has MORE KV cache than the 7B model? And almost as much as the 120B model?\nThis suggests:\nQwen-72B has superior memory optimization Model architecture matters more than parameter count There\u0026rsquo;s room to optimize other models similarly Phase 2 Goal: Understand KV cache scaling and memory efficiency across different model architectures.\nPhase 2: The Factory Reset Experiment I\u0026rsquo;m planning a comprehensive Phase 2 investigation:\nThe Plan Factory reset DGX Spark - Start completely fresh Install bare metal software - Match container versions exactly Create 1:1 configurations - Container vs native, identical software Run extended benchmarks - More models, longer contexts, varied batch sizes Investigate KV cache scaling - Why does Qwen-72B use memory so efficiently? Goals Validate my findings with even cleaner test setup Eliminate any remaining configuration variables Deep dive into KV cache allocation strategies Understand memory efficiency across model families Test on other hardware (discrete GPU comparison) Open Questions Can I optimize Docker for unified memory? (cgroup v2, special configs) Do other unified memory systems (AMD MI300) show the same pattern? How does KV cache scale with context length (8k, 32k, 128k tokens)? What model architectures are most memory-efficient? Share Your Findings If you\u0026rsquo;re running Grace Hopper systems (or other unified memory architectures), I\u0026rsquo;d love to hear from you:\nAre you seeing similar patterns? Have you found workarounds? Do you have additional data to share? GitHub Repo: benchmark-spark\nOpen an issue or submit a PR with your findings!\nResources All the code, data, and analysis are open source:\nüìä Interactive Results: brandonrc.github.io/benchmark-spark üìÑ Full Analysis: ANALYSIS.md üîß Benchmark Scripts: scripts/ üì¶ Raw Data: results/comprehensive/ Final Thoughts This investigation reinforced something fundamental:\nModern AI infrastructure is a stack:\nHardware (Grace Hopper) Kernel (Linux cgroups) Drivers (NVIDIA, CUDA) Runtime (Docker, containerd) Software (TensorRT-LLM, PyTorch) Applications (Your LLM workload) A problem at any layer can look like a problem at any other layer.\nWhen something seems slow or inefficient, resist the urge to blame the most visible component (usually the hardware or the framework). Instead:\nMeasure everything - Get real data Isolate variables - Test different configurations Understand the stack - Know what each layer does Share findings - Help the community The YouTubers who blamed NVIDIA weren\u0026rsquo;t doing engineering. They were doing performance theater.\nI did engineering. And I found the real answer.\nWhat\u0026rsquo;s Your Experience? Have you encountered similar issues? Different findings? Better solutions?\nComment on GitHub Discussions Share your data Help me build Phase 2 Together, you and I can make GPU computing better for everyone - by actually understanding it instead of just pointing fingers.\nPrevious: ‚Üê Part 4: The Data - 60 Runs Don\u0026rsquo;t Lie\nGitHub Repo: benchmark-spark\nPhase 2 Tracking: GitHub Issues\nThanks for following along! üöÄ\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/05-what-we-learned/","summary":"\u003ch2 id=\"the-journey-so-far\"\u003eThe Journey So Far\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s recap this wild ride:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eThe Problem\u003c/strong\u003e: YouTube reviewers blamed NVIDIA hardware for being slow\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe Investigation\u003c/strong\u003e: I found 20-30GB memory overhead in Docker containers\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe Environment Setup\u003c/strong\u003e: MPI and chroot configuration nightmare\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe Revelation\u003c/strong\u003e: Docker\u0026rsquo;s cgroups double-count unified memory\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe Data\u003c/strong\u003e: 60 runs confirmed the pattern consistently\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNow, what do I actually \u003cstrong\u003edo\u003c/strong\u003e with this knowledge?\u003c/p\u003e\n\u003ch2 id=\"key-finding-dont-blame-the-hardware\"\u003eKey Finding: Don\u0026rsquo;t Blame the Hardware\u003c/h2\u003e\n\u003cp\u003eThe most important lesson from this entire investigation:\u003c/p\u003e","title":"What I Learned (And What's Next)"},{"content":"The Comprehensive Test Plan Anecdotes are interesting. Single data points are suggestive. But 60 benchmark runs across multiple models? That\u0026rsquo;s science.\nHere\u0026rsquo;s what I did:\nTest Matrix 3 Models: DeepSeek-R1-Distill-Qwen-7B (7 billion parameters) Qwen2.5-72B-Instruct (72 billion parameters) GPT-OSS-120B (120 billion parameters, MXFP4 quantized) 2 Environments: Native (chroot) vs Container (Docker) 10 Iterations per model per environment Total: 60 benchmark runs Methodology Framework: TensorRT-LLM (trtllm-bench CLI) Workload: 50 requests, 128 output tokens per request Cooldown: 5 minutes + GPU temp check (\u0026lt; 45¬∞C) between runs Duration: ~14 hours total Metrics: Peak memory, KV cache, throughput, latency, temperature DeepSeek-7B Results My smallest model, but the most shocking results:\nMetric Native Container Difference Peak Memory 70.47 GiB 101.30 GiB +30.83 GiB (44% overhead!) KV Cache 44.31 GiB 16.57 GiB -27.74 GiB (63% less!) Throughput 119.79 tok/s 119.40 tok/s 0.3% difference Std Dev (œÉ) 0.55 0.16 Very stable Analysis: The 7B model shows the largest overhead at 30.8GB. Container KV cache is reduced to only 37% of native. That\u0026rsquo;s massive.\nGPT-OSS-120B Results The largest model (120B params, though MoE means 5.1B active):\nMetric Native Container Difference Peak Memory 71.72 GiB 93.43 GiB +21.71 GiB (30% overhead) KV Cache 43.19 GiB 23.65 GiB -19.54 GiB (45% less) Throughput 120.26 tok/s 120.41 tok/s -0.1% difference Std Dev (œÉ) 0.32 0.54 Very stable Analysis: Despite being the largest model, GPT-OSS shows moderate overhead due to MXFP4 quantization. Native still has 1.8x more KV cache.\nQwen2.5-72B Results The 72B parameter model - the sweet spot:\nMetric Native Container Difference Peak Memory 70.03 GiB 90.02 GiB +19.99 GiB (29% overhead) KV Cache 44.71 GiB 26.72 GiB -17.99 GiB (40% less) Throughput 119.33 tok/s 119.51 tok/s -0.2% difference Std Dev (œÉ) 0.28 0.20 Very stable Analysis: Qwen shows the most efficient memory usage of all models. Native mode has the highest KV cache at 44.71 GiB - even more than the 7B model!\nThe Pattern: Container Overhead Scales Look at the overhead across models:\nModel Size Overhead Pattern DeepSeek 7B +30.83 GiB (44%) Highest % Qwen 72B +19.99 GiB (29%) Middle GPT-OSS 120B +21.71 GiB (30%) Middle Insight: Overhead appears proportional to base memory usage, not model size. This suggests Docker\u0026rsquo;s cgroup accounting scales with allocation size, confirming our unified memory double-counting theory.\nPerformance Parity: The Good News Across all 60 runs, performance was virtually identical:\nT - - - h r N C D o a o i u t n f g i t f h v a e p e i r u : n e t e n r c R : e a : n 1 1 g 1 1 \u0026lt; e 9 9 : . . 0 3 4 . 3 0 3 % - - ( 1 1 w 2 2 i 0 0 t . . h 2 5 i 6 1 n t t m o o a k k r e e g n n i s s n / / s s o e e f c c e r r o r ) Standard deviations were incredibly low (œÉ \u0026lt; 0.6 tokens/sec), showing:\nExcellent thermal management Consistent GPU performance No thermal throttling The KV Cache Revelation This is the real story. Across all models:\nModel Native KV Container KV Ratio DeepSeek 44.31 GiB 16.57 GiB 2.7x more GPT-OSS 43.19 GiB 23.65 GiB 1.8x more Qwen 44.71 GiB 26.72 GiB 1.7x more Native mode provides 1.7-2.7x more KV cache across the board. This is huge for:\nLonger context windows Higher batch sizes Better concurrent request handling Overall throughput scaling Temperature Analysis Interestingly, containers ran slightly cooler:\nEnvironment Avg End Temp Range Native 60.6¬∞C 59-61¬∞C Container 58.6¬∞C 57-59¬∞C Why? Container overhead means more time in cooldown between runs. Actual compute time is the same, but total wall time is longer due to additional memory management.\nData Quality and Reproducibility All 60 runs completed successfully with:\n‚úÖ 0 failures - Every benchmark completed ‚úÖ Consistent results - Very low standard deviations ‚úÖ Proper thermal management - No throttling ‚úÖ Comprehensive logging - Full metadata saved Raw data available: GitHub - benchmark-spark/results\nInteractive Charts Want to explore the data visually? Check out our interactive results page with:\nPeak memory comparisons KV cache allocation charts Throughput performance graphs What This Proves After 60 runs across 3 models, the findings are clear:\nContainer overhead is real: 20-30 GB consistently KV cache reduction is significant: 1.7-2.7x less in containers Performance is identical: No speed penalty for native execution Pattern is consistent: Happens across all model sizes Root cause confirmed: Unified memory + cgroup double-counting This isn\u0026rsquo;t a fluke. This isn\u0026rsquo;t a configuration error. This is a fundamental architectural mismatch.\nNext up: What we learned, practical recommendations, and what we\u0026rsquo;re investigating next (spoiler: that KV cache scaling is fascinating).\nPrevious: ‚Üê Part 3: The Unified Memory Revelation\nNext: Part 5: What We Learned (And What\u0026rsquo;s Next) ‚Üí\nGitHub Repo: benchmark-spark\nInteractive Charts: Results Dashboard\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/04-the-data/","summary":"\u003ch2 id=\"the-comprehensive-test-plan\"\u003eThe Comprehensive Test Plan\u003c/h2\u003e\n\u003cp\u003eAnecdotes are interesting. Single data points are suggestive. But \u003cstrong\u003e60 benchmark runs\u003c/strong\u003e across multiple models? That\u0026rsquo;s science.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s what I did:\u003c/p\u003e\n\u003ch3 id=\"test-matrix\"\u003eTest Matrix\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e3 Models\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eDeepSeek-R1-Distill-Qwen-7B (7 billion parameters)\u003c/li\u003e\n\u003cli\u003eQwen2.5-72B-Instruct (72 billion parameters)\u003c/li\u003e\n\u003cli\u003eGPT-OSS-120B (120 billion parameters, MXFP4 quantized)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e2 Environments\u003c/strong\u003e: Native (chroot) vs Container (Docker)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e10 Iterations\u003c/strong\u003e per model per environment\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal\u003c/strong\u003e: 60 benchmark runs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"methodology\"\u003eMethodology\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFramework\u003c/strong\u003e: TensorRT-LLM (\u003ccode\u003etrtllm-bench\u003c/code\u003e CLI)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWorkload\u003c/strong\u003e: 50 requests, 128 output tokens per request\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCooldown\u003c/strong\u003e: 5 minutes + GPU temp check (\u0026lt; 45¬∞C) between runs\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDuration\u003c/strong\u003e: ~14 hours total\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMetrics\u003c/strong\u003e: Peak memory, KV cache, throughput, latency, temperature\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"deepseek-7b-results\"\u003eDeepSeek-7B Results\u003c/h2\u003e\n\u003cp\u003eMy smallest model, but the most shocking results:\u003c/p\u003e","title":"The Data: 60 Runs Don't Lie"},{"content":"The Question That Started It All After getting both Docker and native environments working, I could finally run proper benchmarks. But I kept asking myself:\n\u0026ldquo;Where is the 26GB going?\u0026rdquo;\nIt wasn\u0026rsquo;t CPU overhead - containers don\u0026rsquo;t add 26GB of process memory.\nIt wasn\u0026rsquo;t the Docker daemon - that\u0026rsquo;s tiny.\nIt wasn\u0026rsquo;t duplicate libraries - bind mounts prevent that.\nSo\u0026hellip; where?\nTraditional GPU Systems (The Old Way) Let\u0026rsquo;s start with how most GPU systems work. Take an NVIDIA H100 or A100:\n‚îå ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ D 6 ‚îÄ ‚îÄ C D 4 ‚îÄ ‚îÄ P R - ‚îÄ ‚îÄ U 5 ‚îÄ ‚îÄ R 1 ‚îÄ ‚îÄ ( A 2 ‚îÄ ‚îÄ H M ‚îÄ ‚îÄ o G ‚îÄ ‚îÄ s B ‚îÄ ‚îÄ t ‚îÄ ‚îÄ ) ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îò ‚óÑ ‚îÄ P ‚îÄ C ‚îÄ I ‚îÄ e ‚ñ∫ ‚îå ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ G H 4 ‚îÄ ‚îÄ P B 0 ‚îÄ ‚îÄ U M - ‚îÄ ‚îÄ 8 ‚îÄ ‚îÄ ( ( 0 ‚îÄ ‚îÄ D V ‚îÄ ‚îÄ e R G ‚îÄ ‚îÄ v A B ‚îÄ ‚îÄ i M ‚îÄ ‚îÄ c ) ‚îÄ ‚îÄ e ‚îÄ ‚îÄ ) ‚îÄ ‚îÄ ‚îÄ ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îò Key points:\nCPU has its own RAM (DDR) GPU has its own VRAM (HBM) Data moves between them over PCIe bus They\u0026rsquo;re separate memory spaces When Docker runs on these systems:\nDocker\u0026rsquo;s cgroups manage CPU RAM only GPU VRAM is outside Docker\u0026rsquo;s control nvidia-docker just passes through GPU access No double-counting because they\u0026rsquo;re separate Grace Hopper: The Game Changer Now look at Grace Hopper (our DGX Spark):\n‚îå ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îå ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ G ‚îÄ U ‚îå ‚îÇ ‚îî C ‚îÄ ‚îÄ ‚îÄ r ‚îÄ N ‚îÄ ‚îÄ o ‚îÄ ‚îÄ ‚îÄ a ‚îÄ I ‚îÄ A ‚îÄ h ‚îÄ ‚îÄ ‚îÄ c ‚îÄ F ‚îÄ R ‚îÄ e ‚îÄ ‚îÄ ‚îÄ e ‚îÄ I ‚îÄ M ‚îÄ r ‚îÄ ‚îÄ ‚îÄ ‚îÄ E ‚îÄ ‚îÄ e ‚îÄ ‚îÄ ‚îÄ H ‚îÄ D ‚îÄ C ‚îÄ n ‚îÄ ‚îÄ ‚îÄ o ‚îÄ ‚îÄ P ‚îÄ t ‚îÄ ‚îÄ ‚îÄ p ‚îÄ M ‚îÄ U ‚îÄ , ‚îÄ ‚îÄ ‚îÄ p ‚îÄ E ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ e ‚îÄ M ‚îê ‚îÇ ‚îò C ‚îÄ ‚îÄ ‚îÄ r ‚îÄ O ‚óÑ a ‚îÄ ‚îÄ ‚îÄ ‚îÄ R ‚îÄ c ‚îÄ ‚îÄ ‚îÄ S ‚îÄ Y ‚îÄ h ‚îÄ ‚îÄ ‚îÄ y ‚îÄ ‚ñ∫ e ‚îÄ ‚îÄ ‚îÄ s ‚îÄ ( ‚îå ‚îÇ ‚îî - ‚îÄ ‚îÄ ‚îÄ t ‚îÄ 1 ‚îÄ ‚îÄ C ‚îÄ ‚îÄ ‚îÄ e ‚îÄ 2 ‚îÄ G ‚îÄ o ‚îÄ ‚îÄ ‚îÄ m ‚îÄ 8 ‚îÄ B ‚îÄ h ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ 1 ‚îÄ e ‚îÄ ‚îÄ ‚îÄ ‚îÄ G ‚îÄ 0 ‚îÄ r ‚îÄ ‚îÄ ‚îÄ ‚îÄ B ‚îÄ ‚îÄ e ‚îÄ ‚îÄ ‚îÄ ‚îÄ ) ‚îÄ G ‚îÄ n ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ P ‚îÄ t ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ U ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê ‚îÇ ‚îò ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îò ‚îÄ ‚îÄ ‚îÄ ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îò Revolutionary differences:\nOne memory pool for both CPU and GPU No PCIe transfers - both access the same RAM Coherent at the hardware level The CPU \u0026ldquo;sees\u0026rdquo; GPU allocations and vice versa This is elegant! No more copying between CPU and GPU. It\u0026rsquo;s all one big shared memory space.\nThe Docker cgroup Problem Here\u0026rsquo;s where things go sideways.\nDocker uses Linux cgroups (control groups) to isolate and track container resources:\n1 2 3 # Docker creates a memory cgroup for each container /sys/fs/cgroup/docker/\u0026lt;container_id\u0026gt;/memory.max /sys/fs/cgroup/docker/\u0026lt;container_id\u0026gt;/memory.current On a traditional GPU system, cgroups see:\nCPU RAM: Managed by cgroup ‚úì GPU VRAM: Outside cgroup (invisible) ‚úì On Grace Hopper unified memory, cgroups see:\nThe entire 128GB pool as \u0026ldquo;system RAM\u0026rdquo; ‚úó The Double-Counting Here\u0026rsquo;s what happens when you run a model in a Docker container on Grace Hopper:\nModel loads into unified memory (let\u0026rsquo;s say 70GB) CUDA driver records this as GPU allocation Docker\u0026rsquo;s cgroup sees 70GB of \u0026ldquo;container RAM\u0026rdquo; used TensorRT-LLM tries to allocate KV cache Docker thinks: \u0026ldquo;Container is using 70GB already, only X GB left\u0026rdquo; Reality: That 70GB is THE SAME MEMORY, just counted twice! Result: Docker reserves extra headroom because it thinks GPU memory is separate \u0026ldquo;container RAM\u0026rdquo;, even though it\u0026rsquo;s not.\nThe Evidence Let\u0026rsquo;s look at what we saw:\nNative (Chroot) T M L K o o e V t d a a e v c l l e a s c u p : h n e e i a f k a i l e u l d s o a c m g a e e t m : e o d r : y : 1 7 4 3 1 8 1 7 9 . . . . 3 3 2 6 1 3 6 4 G G G G B B B B ( 9 0 % o f a v a i l a b l e ) Container (Docker) T M L K o o e V t d a a e v c l l e a s c u p : h n e e i a f k a i l e u l d s o a c m g a e e t m : e o d r : y : 1 1 1 1 1 0 4 3 9 4 . . . . 7 3 6 9 2 1 4 2 G G G G B B B B ( 9 ‚Üê 0 % W h o a f t ? a ! v a i l a b l e ) Docker\u0026rsquo;s cgroup sees 104.92GB used, but the actual model only needs 78GB. The difference (26.6GB) is phantom overhead from Docker\u0026rsquo;s memory accounting trying to \u0026ldquo;reserve\u0026rdquo; space that\u0026rsquo;s already in use.\nWhy Isn\u0026rsquo;t This a Problem on Discrete GPUs? On H100/A100 systems, cgroups can\u0026rsquo;t even see GPU VRAM. It\u0026rsquo;s on a separate PCIe device. So there\u0026rsquo;s no double-counting:\nD - - - - U - - - - i n s c C T N i c C D C c g U o o f g U o r r r D t i r D c e e o A a e o A k a t u l v d u e t e p t : e p t r e r r M r ' s G t a 6 l e t a s P r c 4 a m r c U a k p o a k a v : c s + r c s c e k : ‚úì y k : c r s 8 s o h : G 0 ( : P u e P G a n a C U = r \" r t d P a S t i U V 1 c y n ‚úó R 4 e s o g R A 4 t f : A M G H e M B o m t C s p h o o e p R a n n p e A t f l a r M u y r ) \" s s a : a e ( t ( m d e e 1 e ! . l 2 g y 8 1 . G 2 , ( B 8 e ) G 6 . B 4 g G . B , ) 8 0 G B ) The KV Cache Impact This double-counting has a massive effect on KV cache:\nA - - - v a N C D i a o i l t n f a i t f b v a e l e i r e : n e e n f r c o : e r : 3 1 K 7 3 2 V . . . 2 3 8 c 6 1 x a c G G M h B B O e R : E i n n a t i v e m o d e More KV cache means:\nLonger context windows Higher batch sizes More concurrent requests Better overall throughput scaling That 26GB overhead isn\u0026rsquo;t just wasted RAM - it\u0026rsquo;s stolen capacity for serving workloads.\nWhy Performance Is Still The Same You might wonder: \u0026ldquo;If Docker has less KV cache, why is throughput identical?\u0026rdquo;\nGood question! For our specific benchmark (50 requests, 128 output tokens):\nEven 13GB of KV cache was enough We weren\u0026rsquo;t hitting the cache limit Throughput was compute-bound, not memory-bound But in production with:\nLonger contexts (8k, 32k, 128k tokens) Higher batch sizes Many concurrent users That reduced KV cache would absolutely become a bottleneck.\nCan Docker Be Fixed? Maybe? Potential solutions:\nUpdate nvidia-container-toolkit for unified memory awareness Use --memory=unlimited to disable cgroup memory limits Special cgroup configuration for Grace Hopper Wait for Docker/kernel patches that understand unified memory But for now, the simplest solution: Use native execution for large models on Grace Hopper.\nThe Takeaway This isn\u0026rsquo;t Docker being \u0026ldquo;bad\u0026rdquo; or Grace Hopper being \u0026ldquo;broken.\u0026rdquo; It\u0026rsquo;s a mismatch between technology generations:\nDocker\u0026rsquo;s cgroups: Designed for discrete GPU era Grace Hopper: Next-gen unified memory architecture Result: Software assumptions don\u0026rsquo;t match hardware reality And that\u0026rsquo;s why you can\u0026rsquo;t just blame the hardware. The entire stack matters.\nIn the next post, I\u0026rsquo;ll show you the data: 60 comprehensive benchmark runs across 3 different models, proving this pattern holds consistently.\nPrevious: ‚Üê Part 2: MPI and Chroot Nightmare\nNext: Part 4: The Data - 60 Runs Don\u0026rsquo;t Lie ‚Üí\nGitHub Repo: benchmark-spark\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/03-unified-memory-revelation/","summary":"\u003ch2 id=\"the-question-that-started-it-all\"\u003eThe Question That Started It All\u003c/h2\u003e\n\u003cp\u003eAfter getting both Docker and native environments working, I could finally run proper benchmarks. But I kept asking myself:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026ldquo;Where is the 26GB going?\u0026rdquo;\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIt wasn\u0026rsquo;t CPU overhead - containers don\u0026rsquo;t add 26GB of process memory.\u003cbr\u003e\nIt wasn\u0026rsquo;t the Docker daemon - that\u0026rsquo;s tiny.\u003cbr\u003e\nIt wasn\u0026rsquo;t duplicate libraries - bind mounts prevent that.\u003c/p\u003e\n\u003cp\u003eSo\u0026hellip; where?\u003c/p\u003e\n\u003ch2 id=\"traditional-gpu-systems-the-old-way\"\u003eTraditional GPU Systems (The Old Way)\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s start with how most GPU systems work. Take an NVIDIA H100 or A100:\u003c/p\u003e","title":"The Unified Memory Revelation: Why Docker Double-Counts"},{"content":"The Simple Plan (That Wasn\u0026rsquo;t Simple) After seeing the mysterious 26GB memory overhead in Docker, my plan was straightforward:\nExtract the container\u0026rsquo;s filesystem Run the same Python scripts natively Compare the results Done! Ha. Hahahaha. No.\nAttempt 1: Just Run It My first thought: \u0026ldquo;Let\u0026rsquo;s just run the Docker scripts on my system. How hard can it be?\u0026rdquo;\n1 2 python /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py \\ --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B What I got: Runtime nightmare.\nM I C o m U d p D u o A l r e t E N E r o r r t r o F o r o r : u : n n d l o E i r b C r m U o p D r i A : . - s c N o a o . p 4 a m 0 b o : l d e u c l a d e n e n v n o i a t c m e e o d p i e s ' n t d e s e n h t s a e o r c r e t r d e t d _ o l b l j m e ' c t f i l e Of course. The container has TensorRT-LLM, specific CUDA versions, MPI libraries, and about a million other dependencies that my host system doesn\u0026rsquo;t have (or has different versions of).\nOkay, fine. Let\u0026rsquo;s extract the container and use chroot.\nExtracting the Container Docker containers are just fancy tarballs with layers. To extract:\n1 2 3 4 5 6 7 8 # Export the container filesystem docker create --name temp nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev docker export temp \u0026gt; container.tar docker rm temp # Extract to a directory mkdir -p /home/khan/container-rootfs sudo tar -xf container.tar -C /home/khan/container-rootfs Now I have the entire container filesystem at /home/khan/container-rootfs/. Cool!\nThe MPI Library Hunt Begins Time to run something:\n1 sudo chroot /home/khan/container-rootfs python3 /workspace/benchmarks/trtllm_benchmark.py New error:\n[ M T P R I T - E L r L r M o ] r : [ I u ] n d M e P f I i n s e i d z e s : y m 1 b , o l r : a n u k c : s _ 0 c o n f i g _ d o c _ n o p What? Let me check which MPI it\u0026rsquo;s using:\n1 2 which mpirun # /usr/bin/mpirun ‚Üê System MPI! Ah. The chroot is finding the system\u0026rsquo;s MPI (installed at /usr/bin/mpirun) instead of the container\u0026rsquo;s MPI (at /opt/hpcx/ompi/bin/mpirun inside the rootfs).\nThe PATH Dance TensorRT-LLM uses HPC-X OpenMPI from NVIDIA. The container has it at /opt/hpcx/ompi/. But when I chroot, the PATH still points to system binaries first.\nSolution: Explicitly set PATH to prioritize container binaries.\n1 export PATH=\u0026#34;/opt/hpcx/ompi/bin:$PATH\u0026#34; Run again\u0026hellip; new error:\nm p i r u n : e r r o r w h i l e l o a d i n g s h a r e d l i b r a r i e s : l i b u c s . s o . 0 : c a n n o t o p e n s h a r e d o b j e c t f i l e The MPI binary is now correct, but it can\u0026rsquo;t find its libraries!\nThe LD_LIBRARY_PATH Saga The container\u0026rsquo;s MPI needs libraries from /opt/hpcx/ompi/lib/, but LD_LIBRARY_PATH doesn\u0026rsquo;t include it.\n1 export LD_LIBRARY_PATH=\u0026#34;/opt/hpcx/ompi/lib:$LD_LIBRARY_PATH\u0026#34; Run again\u0026hellip; DIFFERENT error:\ns y m b o l l o o k u p e r r o r : o p t / h p c x / o m p i / l i b / l i b u c c . s o . 1 : u n d e f i n e d s y m b o l : u c p _ w o r k e r _ p r o g r e s s Wait, what? Now it\u0026rsquo;s finding the HPC-X libraries but they\u0026rsquo;re conflicting with system libraries!\nThe Real Problem Here\u0026rsquo;s what was happening:\nThe system has OpenMPI installed (libmpi.so) The container has HPC-X OpenMPI (/opt/hpcx/ompi/lib/libmpi.so) TensorRT-LLM needs the HPC-X version But the dynamic linker was mixing system and container libraries The fix: Put container libraries at the FRONT of LD_LIBRARY_PATH:\n1 export LD_LIBRARY_PATH=\u0026#34;/opt/hpcx/ompi/lib:/usr/local/lib/python3.12/dist-packages/tensorrt_libs:$LD_LIBRARY_PATH\u0026#34; Finally, MPI works!\nThe CUDA Version Surprise Got MPI working. Started a benchmark. New error:\nR u n t i m e E r r o r : T r i t o n o n l y s u p p o r t s C U D A 1 0 . 0 o r h i g h e r , b u t g o t C U D A v e r s i o n : 1 3 . 0 Wait, CUDA 13.0? We\u0026rsquo;re using CUDA 12.9!\nTurns out: The system symlink /usr/local/cuda pointed to CUDA 13.0. The container needs CUDA 12.9.\nFix:\n1 2 export CUDA_HOME=\u0026#34;/usr/local/cuda-12.9\u0026#34; export PATH=\u0026#34;/usr/local/cuda-12.9/bin:$PATH\u0026#34; The Full Chroot Wrapper Script After all this pain, I created a proper chroot wrapper that handles everything:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash SCRIPT_DIR=\u0026#34;/home/khan/container-rootfs\u0026#34; # Mount necessary filesystems sudo mount -t proc /proc \u0026#34;${SCRIPT_DIR}/proc\u0026#34; sudo mount --rbind /sys \u0026#34;${SCRIPT_DIR}/sys\u0026#34; sudo mount --rbind /dev \u0026#34;${SCRIPT_DIR}/dev\u0026#34; sudo mount --bind /home \u0026#34;${SCRIPT_DIR}/home\u0026#34; sudo mount --bind /data \u0026#34;${SCRIPT_DIR}/data\u0026#34; # DNS resolution sudo cp /etc/resolv.conf \u0026#34;${SCRIPT_DIR}/etc/resolv.conf\u0026#34; # Run in chroot with proper environment sudo chroot \u0026#34;${SCRIPT_DIR}\u0026#34; /usr/bin/env -i \\ HOME=/root \\ PATH=\u0026#34;/opt/hpcx/ompi/bin:/usr/local/cuda-12.9/bin:/usr/local/bin:/usr/bin:/bin\u0026#34; \\ LD_LIBRARY_PATH=\u0026#34;/opt/hpcx/ompi/lib:/usr/local/lib/python3.12/dist-packages/tensorrt_libs:/usr/local/cuda-12.9/lib64\u0026#34; \\ CUDA_HOME=\u0026#34;/usr/local/cuda-12.9\u0026#34; \\ PYTHONPATH=\u0026#34;/usr/local/lib/python3.12/dist-packages\u0026#34; \\ \u0026#34;$@\u0026#34; # Cleanup: unmount everything sudo umount \u0026#34;${SCRIPT_DIR}/data\u0026#34; sudo umount \u0026#34;${SCRIPT_DIR}/home\u0026#34; sudo umount \u0026#34;${SCRIPT_DIR}/dev\u0026#34; sudo umount \u0026#34;${SCRIPT_DIR}/sys\u0026#34; sudo umount \u0026#34;${SCRIPT_DIR}/proc\u0026#34; Now I can run:\n1 ./run_in_rootfs.sh python3 /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py And it actually works.\nLessons Learned Library paths matter: System vs container libraries will bite you Environment is everything: PATH, LD_LIBRARY_PATH, CUDA_HOME all critical MPI is picky: HPC-X OpenMPI isn\u0026rsquo;t interchangeable with system OpenMPI Filesystem mounts: Need /proc, /sys, /dev, /home, and /data bind mounts DNS matters: Even forgot /etc/resolv.conf initially! The Hard Part Is Over\u0026hellip; Right? With a working chroot environment, I could finally start benchmarking. But getting here took hours of debugging library paths and runtime errors.\nSometimes I wonder if this is why people just accept Docker\u0026rsquo;s overhead - at least it works out of the box!\nBut now that we have both Docker and native environments working, we can actually compare them fairly. And that\u0026rsquo;s where things get interesting\u0026hellip;\nIn the next post: What Grace Hopper\u0026rsquo;s unified memory architecture actually means, and why Docker\u0026rsquo;s cgroups don\u0026rsquo;t understand it.\nPrevious: ‚Üê Part 1: The Mystery\nNext: Part 3: The Unified Memory Revelation ‚Üí\nGitHub Repo: benchmark-spark\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/02-mpi-chroot-nightmare/","summary":"\u003ch2 id=\"the-simple-plan-that-wasnt-simple\"\u003eThe Simple Plan (That Wasn\u0026rsquo;t Simple)\u003c/h2\u003e\n\u003cp\u003eAfter seeing the mysterious 26GB memory overhead in Docker, my plan was straightforward:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExtract the container\u0026rsquo;s filesystem\u003c/li\u003e\n\u003cli\u003eRun the same Python scripts natively\u003c/li\u003e\n\u003cli\u003eCompare the results\u003c/li\u003e\n\u003cli\u003eDone!\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHa. Hahahaha. \u003cstrong\u003eNo.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"attempt-1-just-run-it\"\u003eAttempt 1: Just Run It\u003c/h2\u003e\n\u003cp\u003eMy first thought: \u0026ldquo;Let\u0026rsquo;s just run the Docker scripts on my system. How hard can it be?\u0026rdquo;\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eWhat I got: \u003cstrong\u003eRuntime nightmare\u003c/strong\u003e.\u003c/p\u003e","title":"Down the Rabbit Hole: The MPI and Chroot Nightmare"},{"content":"The YouTube Problem If you search for \u0026ldquo;DGX Spark performance\u0026rdquo; on YouTube, you\u0026rsquo;ll find plenty of videos with clickbait titles like \u0026ldquo;NVIDIA\u0026rsquo;s $X Machine is a DISAPPOINTMENT\u0026rdquo; or \u0026ldquo;Grace Hopper: Overhyped and Underdelivering.\u0026rdquo;\nAnd that really bothers me.\nNot because I\u0026rsquo;m an NVIDIA fanboy (I\u0026rsquo;m not), but because none of these reviewers provided a technical explanation of why performance wasn\u0026rsquo;t meeting expectations. They just pointed at benchmark numbers, said \u0026ldquo;slow,\u0026rdquo; and moved on. No investigation into kernel settings, driver versions, container configurations, or software stack optimization. Just\u0026hellip; blame the hardware.\nThat\u0026rsquo;s not how engineering works.\nMy Setup: The New Kid on the Block I got my hands on an NVIDIA DGX Spark - a genuinely interesting piece of hardware:\nCPU: ARM Cortex (X925 + A725), 20 cores total GPU: NVIDIA GB10 (Blackwell architecture, Grace Hopper design) Memory: 128GB unified (CPU and GPU share the same RAM pool) Architecture: aarch64 (ARM64) The unified memory part is key. Unlike traditional systems where you have separate CPU RAM and GPU VRAM that copy data back and forth over PCIe, Grace Hopper has one coherent memory space. Both processors can access the same 128GB directly.\nPretty elegant, right?\nPrior Art: Docker GPU Passthrough Overhead Before diving into our own investigation, I found this paper: Benchmarking GPU Passthrough Performance on Docker for AI Cloud System.\nThe authors tested Docker GPU passthrough on consumer hardware (RTX 3060) and found:\nNative execution: Faster (1.52s avg) Docker containers: Slower (2.55s avg), but higher GPU utilization This validated that Docker overhead exists. But that study used consumer GPUs with simple matrix multiplication. I wanted to understand:\nDoes this apply to enterprise DGX hardware? What about production LLM workloads, not just matmul? WHY does this overhead exist in my specific case? The Test I ran TensorRT-LLM inference benchmarks in two environments:\nDocker Container (NVIDIA\u0026rsquo;s official image):\n1 2 3 docker run --rm --gpus all --ipc=host --shm-size=60g \\ nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev \\ python benchmarks/trtllm_benchmark.py Native (Chroot): Same code, same libraries, but running directly on the host using chroot to access the container\u0026rsquo;s filesystem.\nThe Shocking Result Environment Peak Memory KV Cache Available Docker Container 104.92 GiB 13.31 GiB Native (Chroot) 78.31 GiB 37.26 GiB Wait\u0026hellip; what?\nThe container was using 26.6 GB MORE memory and had less than half the KV cache available for the model!\nBut\u0026hellip; Performance Was Identical Here\u0026rsquo;s what made this even more confusing:\nN C a o t n i t v a e i : n e r : 1 1 2 2 0 1 . . 4 6 4 0 t t o o k k e e n n s s / / s s e e c c Same throughput. Same latency. The container wasn\u0026rsquo;t slower, it was just using way more memory for no apparent reason.\nThis wasn\u0026rsquo;t a \u0026ldquo;slow hardware\u0026rdquo; problem. This was something deeper.\nWhy This Matters You might think \u0026ldquo;who cares about 26GB if performance is the same?\u0026rdquo; But:\nKV Cache: That 26GB could enable longer contexts or higher batch sizes Multi-tenancy: Less memory per container = fewer workloads per box Cost: In cloud deployments, you pay for memory Principle: When hardware isn\u0026rsquo;t the problem, blaming hardware is lazy The Investigation Begins Rather than accept this as \u0026ldquo;Docker is bloated\u0026rdquo; or \u0026ldquo;Grace Hopper is broken,\u0026rdquo; I decided to dig in:\nRun comprehensive benchmarks (60 runs across 3 different model sizes) Test multiple LLMs: 7B, 72B, and 120B parameter models Understand what Grace Hopper\u0026rsquo;s unified memory architecture really means Figure out exactly where that 26GB is going The journey involved:\nMPI library path hell Chroot filesystem nightmares A revelation about Linux cgroups and unified memory Some genuinely surprising discoveries about KV cache scaling Spoiler: I found the root cause. And it\u0026rsquo;s not what you think.\nIn the next post, I\u0026rsquo;ll walk through the pain of setting up a proper test environment. Because sometimes the hardest part of debugging isn\u0026rsquo;t finding the bug - it\u0026rsquo;s just getting to a clean test in the first place.\nNext: Part 2: Down the Rabbit Hole - MPI and Chroot ‚Üí\nGitHub Repo: benchmark-spark\nReferences:\nBenchmarking GPU Passthrough Performance on Docker for AI Cloud System ","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/01-the-mystery/","summary":"\u003ch2 id=\"the-youtube-problem\"\u003eThe YouTube Problem\u003c/h2\u003e\n\u003cp\u003eIf you search for \u0026ldquo;DGX Spark performance\u0026rdquo; on YouTube, you\u0026rsquo;ll find plenty of videos with clickbait titles like \u0026ldquo;NVIDIA\u0026rsquo;s $X Machine is a DISAPPOINTMENT\u0026rdquo; or \u0026ldquo;Grace Hopper: Overhyped and Underdelivering.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eAnd that really bothers me.\u003c/p\u003e\n\u003cp\u003eNot because I\u0026rsquo;m an NVIDIA fanboy (I\u0026rsquo;m not), but because \u003cstrong\u003enone of these reviewers provided a technical explanation\u003c/strong\u003e of why performance wasn\u0026rsquo;t meeting expectations. They just pointed at benchmark numbers, said \u0026ldquo;slow,\u0026rdquo; and moved on. No investigation into kernel settings, driver versions, container configurations, or software stack optimization. Just\u0026hellip; blame the hardware.\u003c/p\u003e","title":"The Mystery: Don't Just Blame the Hardware"},{"content":"Who \u0026amp; Why I\u0026rsquo;m Brandon Geraci, and I was tired of seeing YouTube tech reviewers blame NVIDIA\u0026rsquo;s DGX Spark hardware for being \u0026ldquo;slow\u0026rdquo; or \u0026ldquo;disappointing\u0026rdquo; without providing any technical analysis.\nSo I decided to actually investigate what was going on.\nWhat We Found After 60 comprehensive benchmarks across 3 different LLM models, we discovered:\nDocker containers use 20-30 GB more memory than native execution on Grace Hopper KV cache is reduced by 40-63% in containers Performance is identical - same throughput, no speed penalty Root cause: Docker\u0026rsquo;s cgroups double-count unified memory This isn\u0026rsquo;t a hardware problem. It\u0026rsquo;s a software stack mismatch.\nThe Project GitHub Repository: benchmark-spark\nAll code, data, and analysis are open source. We ran:\n60 total benchmark runs 3 models: DeepSeek-7B, Qwen-72B, GPT-OSS-120B 2 environments: Native (chroot) vs Container (Docker) 10 iterations per configuration ~14 hours of comprehensive testing Phase 2: What\u0026rsquo;s Next We\u0026rsquo;re planning a follow-up investigation:\nFactory reset the DGX Spark Create 1:1 bare metal vs container configs Deep dive into KV cache scaling Test on discrete GPU systems for comparison Get Involved If you\u0026rsquo;re running Grace Hopper or other unified memory systems:\nShare your findings Open issues on GitHub Contribute data or analysis Let\u0026rsquo;s understand this together - with engineering, not speculation.\nContact: GitHub\nProject: benchmark-spark\nInteractive Results: Results Dashboard\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/about/","summary":"\u003ch2 id=\"who--why\"\u003eWho \u0026amp; Why\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m Brandon Geraci, and I was tired of seeing YouTube tech reviewers blame NVIDIA\u0026rsquo;s DGX Spark hardware for being \u0026ldquo;slow\u0026rdquo; or \u0026ldquo;disappointing\u0026rdquo; without providing any technical analysis.\u003c/p\u003e\n\u003cp\u003eSo I decided to actually investigate what was going on.\u003c/p\u003e\n\u003ch2 id=\"what-we-found\"\u003eWhat We Found\u003c/h2\u003e\n\u003cp\u003eAfter 60 comprehensive benchmarks across 3 different LLM models, we discovered:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDocker containers use 20-30 GB more memory\u003c/strong\u003e than native execution on Grace Hopper\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKV cache is reduced by 40-63%\u003c/strong\u003e in containers\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePerformance is identical\u003c/strong\u003e - same throughput, no speed penalty\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRoot cause\u003c/strong\u003e: Docker\u0026rsquo;s cgroups double-count unified memory\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a hardware problem. It\u0026rsquo;s a software stack mismatch.\u003c/p\u003e","title":"About This Investigation"}]