[{"content":"The Pattern That Demands Explanation In Part 4, I showed you 60 benchmark runs that revealed a consistent pattern. But one table in particular kept me up at night:\nModel Native KV Container KV Reduction Factor DeepSeek-7B 44.31 GiB 16.57 GiB 2.7x less GPT-OSS-120B 43.19 GiB 23.65 GiB 1.8x less Qwen2.5-72B 44.71 GiB 26.72 GiB 1.7x less Two questions immediately jump out:\nWhy do containers consistently allocate ~2x less KV cache? (The main mystery) Why do all native runs converge to ~44 GiB? (The secondary puzzle) Let\u0026rsquo;s answer both.\nQuestion 1: Why ~2x Less KV Cache in Containers? The High-Level Answer From Part 3, you know Docker\u0026rsquo;s cgroups double-count unified memory. But how does that translate to exactly 40-60% less KV cache?\nThe answer lies in how TensorRT-LLM allocates KV cache memory.\nTensorRT-LLM\u0026rsquo;s Memory Budget Calculation When TensorRT-LLM starts up, it performs a memory budget calculation:\n1 2 3 4 5 . . . . . Q L A C A u o l a l e a l l l r d o c o y c u c m a l a t o t a t o d e t e t e e a l s K l p r V w a e a e c m c v i e a a a g i c i h f n h l t o i e a s r n b g = l i a e n c \" f t t f ( m o i r r e v e e m u a e m o n t \" a r i i i y f o m n i n e i e s m n d o g a r m n y m e d e m m o t o r e r y m y p ) o r a r y b u f f e r s Step 1 is where Docker breaks everything.\nNative Execution: Clean Memory View In native/chroot execution, TensorRT-LLM queries memory directly via CUDA:\nâ”Œ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”” â”€ â”€ â”€ â”€ â”€ T â”Œ â”‚ â”‚ â”‚ â”” T â”€ â”€ o â”€ â”€ e â”€ â”€ t â”€ â”€ n â”€ â”€ a â”€ â”€ s â”€ â”€ l â”€ M W ~ â”€ o â”€ â”€ â”€ o e 1 â”€ r â”€ â”€ G â”€ d i 5 â”€ R â”€ â”€ r â”€ e g â”€ T â”€ â”€ a â”€ l h G â”€ - â”€ â”€ c â”€ t B â”€ L â”€ â”€ e â”€ s â”€ L â”€ â”€ â”€ â”€ M â”€ â”€ H â”€ â”€ â”€ â”€ o â”€ â”€ s â”€ â”€ p â” â”‚ â”‚ â”‚ â”˜ e â”€ â”€ p e â”€ â”€ e s â”€ â”€ r â”Œ â”‚ â”‚ â”‚ â”” : â”€ â”€ â”€ â”€ â”€ â”€ U â”€ â”€ 1 â”€ â”€ n â”€ S R â”€ 0 â”€ â”€ i â”€ y e ~ â”€ 5 â”€ â”€ f â”€ s s 8 â”€ â”€ â”€ i â”€ t e â”€ G â”€ â”€ e â”€ e r G â”€ B â”€ â”€ d â”€ m v B â”€ â”€ â”€ â”€ e â”€ a â”€ â”€ M â”€ â”€ v â”€ â”€ e â” â”‚ â”‚ â”‚ â”˜ a â”€ â”€ m i â”€ â”€ o l â”€ â”€ r â”Œ â”‚ â”‚ â”‚ â”” a â”€ â”€ y â”€ â”€ b â”€ â”€ : â”€ ~ â”€ l â”€ â”€ â”€ F 1 â”€ e â”€ â”€ 1 â”€ R 0 â”€ â”€ â”€ 2 â”€ E 5 â”€ â”€ â”€ 8 â”€ E G â”€ â”€ â”€ â”€ B â”€ â”€ â”€ G â”€ â”€ â”€ â”€ B â” â”‚ â”‚ â”‚ â”˜ â”€ â”€ â”€ â”€ â”‚ â”‚ â”‚ â”‚ â”‚ â”€ â” â”‚ â”‚ â”‚ â”‚ â”˜ Native allocation logic:\nTotal available: ~105 GB Model + activations: ~60-65 GB Remaining: ~40-45 GB KV cache allocated: ~44 GB âœ“ Clean, simple, accurate.\nContainer Execution: Docker\u0026rsquo;s Interference In Docker containers, TensorRT-LLM still uses CUDA APIs, but Docker\u0026rsquo;s cgroup accounting pollutes the results:\nâ”Œ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”” â”€ â”€ â”€ â”€ â”€ T â”Œ â”‚ â”‚ â”‚ â”” D \" â”Œ â”‚ â”‚ â”‚ â”” T ( â”€ â”€ o â”€ â”€ o c â”€ â”€ e R â”€ â”€ t â”€ â”€ c o â”€ â”€ n e â”€ â”€ a â”€ â”€ k n â”€ D \" \" â”€ s d â”€ â”€ l â”€ M W ~ â”€ e t â”€ o C O â”€ o u â”€ â”€ â”€ o e 1 â”€ â†“ r a â†“ â”€ c o n â”€ r c â”€ â”€ U â”€ d i 5 â”€ i â”€ k n l â”€ R e â”€ â”€ n â”€ e g â”€ c n â”€ e t y â”€ T d â”€ â”€ i â”€ l h G â”€ g e â”€ r a â”€ - â”€ â”€ f â”€ t B â”€ r r â”€ ' i 5 â”€ L b â”€ â”€ i â”€ s â”€ o â”€ s n 8 â”€ L y â”€ â”€ e â”€ â”€ u R â”€ e G â”€ M â”€ â”€ d â”€ â”€ p A â”€ v r B â”€ D â”€ â”€ â”€ â”€ M â”€ i â”€ s o â”€ â”€ M â” â”‚ â”‚ â”‚ â”˜ s â”€ e u l â”€ e c â”€ â”€ e e u â”€ w s e â”€ e k â”€ â”€ m e s â”€ : i f â”€ s e â”€ â”€ o â”Œ â”‚ â”‚ â”‚ â”” s a â”€ n t â”€ : r â”€ â”€ r â”€ â”€ g â”€ g â”€ ' â”€ â”€ y â”€ â”€ t e â”€ f â”€ ~ s â”€ â”€ : â”€ S R â”€ h \" â”€ 7 o â”€ 6 â”€ â”€ â”€ y e ~ â”€ i â”€ 0 r â”€ 0 a â”€ â”€ 1 â”€ s s 8 â”€ s â”€ G â”€ c â”€ â”€ 2 â”€ t e â”€ â”€ B a â”€ G c â”€ â”€ 8 â”€ e r G â”€ a â”€ l â”€ B o â”€ â”€ â”€ m v B â”€ s â”€ R l â”€ u â”€ â”€ G â”€ e â”€ â”€ A o â”€ a n â”€ â”€ B â”€ â”€ â”€ M c â”€ v t â”€ â”€ â” â”‚ â”‚ â”‚ â”˜ â”€ \" \" â”€ a i â”€ â”€ â”€ â”€ i n â”€ â”€ â”€ â”€ l g â”€ â”€ â”Œ â”‚ â”‚ â”‚ â”” â” â”‚ â”‚ â”‚ â”˜ a ! â”€ â”€ â”€ â”€ b ) â”€ â”€ â”€ ~ â”€ l â”€ â”€ â”€ F 1 â”€ e â”€ â”€ â”€ R 0 â”€ â”€ â”€ â”€ E 5 â”€ â”€ â”€ â”€ E G â”€ â”€ â”€ â”€ B â”€ â”€ â”€ â”€ â”€ â”€ â”€ â” â”‚ â”‚ â”‚ â”˜ â”€ â”€ â”€ â”€ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”€ â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”˜ Container allocation logic:\nDocker reports: ~60 GB available (wrong!) Model + activations: ~60-65 GB Remaining: ~0-5 GB (uh oh\u0026hellip;) KV cache allocated: ~16-26 GB (conservative!) TensorRT-LLM plays it safe and allocates less KV cache to avoid OOM errors.\nThe Math: Where Does the Overhead Come From? Look at this correlation from Part 5:\nModel Container Overhead KV Cache Reduction DeepSeek-7B +30.83 GiB -27.74 GiB Qwen-72B +19.99 GiB -17.99 GiB GPT-OSS-120B +21.71 GiB -19.54 GiB Notice: Container overhead â‰ˆ KV cache reduction (within 1-3 GB)\nThis isn\u0026rsquo;t a coincidence. Here\u0026rsquo;s what\u0026rsquo;s happening:\nNative memory allocation:\nT S M W K A â”€ T o y o o V v â”€ o t s d r a â”€ t a t e k C i â”€ a l e l s a l â”€ l : m : p c a â”€ : : a h b â”€ c e l â”€ e : e â”€ : : â”€ â”€ 1 ~ ~ ~ ~ â”€ 1 2 ~ 1 4 4 1 â”€ 2 8 8 5 5 4 6 â”€ 8 â”€ G G G G G G â”€ G B B B B B B B ( ( ( ( âœ“ O w a s S e c a , i t f g i e d h v t r t a y i s t v ) i m e o a r n r s s g , , i n e t ) t e c m . p ) b u f f e r s ) Container memory allocation:\nT S M D W K A â”€ T o y o o o V v â”€ o t s d c r a â”€ t a t e k k C i â”€ a l e l e s a l â”€ l : m : r p c a â”€ : : : a h b â”€ c e l â”€ e : e â”€ : : â”€ â”€ 1 ~ + ~ ~ ~ â”€ 1 2 ~ 1 2 4 1 1 â”€ 2 8 8 5 0 5 6 4 â”€ 8 - - - â”€ G G G 3 G 2 2 â”€ G B B B 0 B 6 4 B ( ( G ( G G âœ“ O w B a B B S e c , i ( t ( ( g p i r s d h h v e a r t a a d m i s n t u e v ) t i c e o o e s r m n d a s s ! f , , ) e v t e e t y t r e c h m m . e p a ) a r d b g u i f f n r f ) o e m r s d ) o u b l e - c o u n t i n g ! ) Docker\u0026rsquo;s double-counting creates phantom overhead that steals memory from the KV cache budget.\nWhy Not 50% Exactly? You might expect exactly 50% reduction if Docker was reporting half the memory. But the reduction varies (1.7x - 2.7x) because:\nBase model size differs - Smaller models have proportionally more KV cache Docker overhead scales - Larger allocations = more double-counting TensorRT-LLM is conservative - It leaves safety margins The pattern: Smaller models suffer worse because the fixed Docker overhead consumes a larger percentage of their memory budget.\nQuestion 2: Why Do All Natives Converge to ~44 GiB? This is the secondary mystery that caught my attention:\nModel Size Parameters Native KV Cache DeepSeek 7B 44.31 GiB Qwen 72B 44.71 GiB GPT-OSS 120B 43.19 GiB Wait\u0026hellip; the 7B model uses essentially the same KV cache as the 120B model?\nWhy This Seems Wrong Intuitively, you\u0026rsquo;d expect:\nLarger models â†’ More parameters â†’ More layers â†’ More KV cache needed But that\u0026rsquo;s not what the data shows!\nThe Explanation: Available Memory Ceiling TensorRT-LLM doesn\u0026rsquo;t allocate KV cache based on model size. It allocates based on available memory after loading the model.\nLet\u0026rsquo;s look at the native memory breakdown:\nDeepSeek-7B (Small model):\nT M A S A â”€ K S o o c y v â”€ V a t d t s a â”€ f a e i t i â”€ c e l l v e l â”€ a t a m a â”€ c y m w t b â”€ h e e i l â”€ e m m i o v e â”€ a o g n e : â”€ a r r h s r â”€ l g y t : h â”€ l i : s e â”€ o n : a â”€ c : d â”€ a : â”€ t â”€ e â”€ d â”€ : â”€ 1 ~ ~ ~ ~ â”€ 4 ~ 2 7 1 8 1 â”€ 4 5 8 0 0 â”€ . 8 G G 3 â”€ 3 G B G B â”€ 1 G B B G â”€ B ( B â”€ G s â”€ B m a ( l 4 l 3 ! % ) o f a v a i l a b l e ) Qwen2.5-72B (Large model):\nT M A S A â”€ K S o o c y v â”€ V a t d t s a â”€ f a e i t i â”€ c e l l v e l â”€ a t a m a â”€ c y m w t b â”€ h e e i l â”€ e m m i o v e â”€ a o g n e : â”€ a r r h s r â”€ l g y t : h â”€ l i : s e â”€ o n : a â”€ c : d â”€ a : â”€ t â”€ e â”€ d â”€ : â”€ 1 ~ ~ ~ ~ â”€ 4 M 2 4 3 8 4 â”€ 4 i 8 5 5 0 â”€ . n G â”€ 7 i G G G B G â”€ 1 m B B B B â”€ a â”€ G l ( â”€ B l a ( r 1 g 1 e 2 ! % ) o f c a l c u l a t e d a v a i l a b l e ! ) Wait, that math doesn\u0026rsquo;t work\u0026hellip;\nThe Real Answer: TensorRT-LLM\u0026rsquo;s Allocation Strategy After analyzing the pattern, I believe TensorRT-LLM uses a ceiling-based allocation strategy:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Pseudocode for TensorRT-LLM KV cache allocation def allocate_kv_cache(): total_memory = query_total_unified_memory() # ~128 GB # Load model first model_memory = load_model_weights() # Calculate available available = total_memory - model_memory - system_reserve # Allocate KV cache with ceiling max_kv_cache = 44 GB # Hardcoded or configured ceiling? kv_cache = min(available * 0.6, max_kv_cache) return kv_cache This would explain why:\nSmall models: Have tons of available memory, but hit the 44 GB ceiling Large models: Have less available memory, but still allocate close to 44 GB Why a Ceiling Exists There are good engineering reasons for a KV cache ceiling:\nPrevents memory thrashing - Too large a cache can hurt performance Reserves memory for batching - Need room for concurrent requests Conservative defaults - Better to be safe than OOM Hardware limitations - Memory bandwidth or cache line considerations The 72B Anomaly Notice Qwen-72B has the highest native KV cache (44.71 GB). This might be because:\nIts model architecture is more memory-efficient Quantization or sparsity reduces weight memory TensorRT-LLM optimizations for this model family But it\u0026rsquo;s still very close to the ~44 GB ceiling.\nWhat This Means for Production Understanding these two patterns has huge implications:\n1. Container KV Cache Reduction Matters More at Scale For small workloads (like my benchmark: 50 requests, 128 tokens), even 16 GB of KV cache is enough.\nBut in production:\n8k context window: You\u0026rsquo;ll hit the limit fast 32k context window: Containers will OOM or refuse requests 128k context window: Forget it - you need native execution The 2x KV cache reduction becomes a 2x throughput bottleneck at scale.\n2. Model Size Doesn\u0026rsquo;t Predict KV Cache Availability The ~44 GB convergence means:\nSmall models (7B) don\u0026rsquo;t get \u0026ldquo;extra\u0026rdquo; KV cache just because they\u0026rsquo;re small Large models (72B+) aren\u0026rsquo;t starved of KV cache just because they\u0026rsquo;re large Recommendation: Choose models based on:\nAccuracy requirements (obviously) Weight memory vs KV cache tradeoff Don\u0026rsquo;t assume bigger = less memory for serving 3. Docker is OK for Specific Use Cases If you\u0026rsquo;re running:\nShort contexts (â‰¤2k tokens) Low concurrency (1-4 simultaneous requests) Small batches Then Docker\u0026rsquo;s reduced KV cache might be acceptable. But know you\u0026rsquo;re leaving 40-60% of serving capacity on the table.\n4. Native Execution for Maximum Throughput For production serving with:\nLong contexts (8k+) High concurrency (10+ simultaneous users) Large batch sizes Use native/chroot execution. The 2x KV cache advantage translates directly to 2x serving capacity.\nVisual Summary Let me show you the full picture:\nâ”Œ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”” â”Œ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”” â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ N â”Œ â”‚ â”‚ â”” C â”€ â”€ C â”Œ â”‚ â”‚ â”‚ â”” D â”€ â”€ A â”€ â”€ l â”€ â”€ O â”€ â”€ o â”€ â”€ T â”€ M 7 â”€ e â”€ â”€ N â”€ M 7 â”€ c â”€ â”€ I â”€ o - â”€ a â”€ â”€ T â”€ o - â”€ k â”€ â”€ V â”€ d 4 â”€ n â”€ â”€ A â”€ d 4 â”€ e â”€ â”€ E â”€ e 5 â”€ â”€ â”€ I â”€ e 5 â”€ r â”€ â”€ â”€ l â”€ m â”€ â”€ N â”€ l â”€ â”€ â”€ E â”€ G â”€ e â”€ â”€ E â”€ G â”€ â”€ â”€ X â”€ B â”€ m â”€ â”€ R â”€ B â”€ v â”€ â”€ E â”€ â”€ o â”€ â”€ â”€ â”€ e â”€ â”€ C â”€ â”€ r â”€ â”€ E â”€ â”€ r â”€ â”€ U â”¬ â”‚ â”‚ â”´ y â”€ â”€ X â”¬ â”‚ â”‚ â”‚ â”´ h â”€ â”€ T â”€ â”€ â”€ â”€ E â”€ â”€ e â”€ â”€ I â”€ A â”€ v â”€ â”€ C â”€ D P + â”€ a â”€ â”€ O â”€ c 1 â”€ i â”€ â”€ U â”€ o h 2 â”€ d â”€ â”€ N â”€ t 0 â”€ e â”€ â”€ T â”€ c a 0 â”€ â”€ â”€ â”€ i - â”€ w â”€ â”€ I â”€ k n - â”€ = â”€ â”€ ( â”€ v 3 â”€ â”€ â”€ O â”€ e t 3 â”€ â”€ â”€ O â”€ a 5 â”€ = â”€ â”€ N â”€ r o 0 â”€ S â”€ â”€ p â”€ t â”€ â”€ â”€ â”€ m G â”€ t â”€ â”€ t â”€ i G â”€ M â”€ â”€ ( â”€ B â”€ o â”€ â”€ i â”€ o B â”€ a â”€ â”€ R â”€ â”€ l â”€ â”€ m â”€ n â”€ x â”€ â”€ e â”¬ â”‚ â”‚ â”‚ â”´ e â”€ â”€ a â”€ s â”€ i â”€ â”€ d â”€ â”€ n â”€ â”€ l â”€ â”€ m â”€ â”€ u â”€ A â”€ â”€ â”€ ) â”¬ â”‚ â”‚ â”´ u â”€ â”€ c â”€ c 1 â”€ K â”€ â”€ â”€ â”€ m â”€ â”€ e â”€ t 0 â”€ V â”€ â”€ â”€ â”€ â”€ â”€ d â”€ i - â”€ â”€ â”€ â”€ K â–ˆ â”€ K â”€ â”€ ) â”€ v 3 â”€ c â”€ â”€ â”€ V â–ˆ â”€ V â”€ â”€ â”€ a 5 â”€ a â”€ â”€ â”€ â–ˆ â”€ â”€ â”€ â”€ t â”€ c â”€ â”€ â”€ C â–ˆ â”€ c â”€ â”€ â”€ i G â”€ h â”€ â”€ â”€ a â–ˆ â”€ a â”€ â”€ â”€ o B â”€ e â”€ â”€ â”€ c â–ˆ â”€ c â”€ â”€ â”€ n â”€ â”€ â”€ â”€ h â–ˆ â”€ h â”€ â”€ â”€ s â”€ b â”€ â”€ â”€ e â–ˆ â”€ e â”€ â”€ â”€ â”€ u â”€ â”€ â”€ â–ˆ â”€ â”€ â”€ â”¬ â”‚ â”‚ â”‚ â”´ d â”€ â”€ â”€ ( â–ˆ â”€ â”€ â”€ â”€ â”€ g â”€ â”€ â”€ ~ â–ˆ â”€ â”€ â”€ â”€ K ( â”€ e â”€ â”€ â”€ 4 â–ˆ â”€ â”€ â”€ â”€ V ~ â–ˆ â”€ t â”€ â”€ â”€ 4 â–ˆ â”€ â”€ â”€ â”€ 2 â–ˆ â”€ â”€ â”€ â”€ â–ˆ â”€ â”€ â”€ â”€ C 0 â–ˆ â”€ â”€ â”€ â”€ G â–ˆ â”€ â”€ â”€ â”€ a â–ˆ â”€ â”€ â”€ â”€ B â–ˆ â”€ â”€ â”€ â”€ c G â”€ â”€ â”€ â”€ ) â”€ â”€ â”€ â”€ h B â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ e ) â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â” â”‚ â”‚ â”˜ â”€ â”€ â” â”‚ â”‚ â”˜ â”€ â”€ â”‚ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”‚ â”‚ â”‚ â”‚ â”‚ â”€ â”€ â”‚ â”‚ â”‚ â”‚ â”€ â”€ â”‚ â”€ â”€ â”‚ â”‚ â”‚ â”€ â” â”˜ â” â”˜ Key Takeaways On the ~2x container reduction:\nDocker\u0026rsquo;s cgroup double-counts unified memory TensorRT-LLM sees \u0026ldquo;less available\u0026rdquo; memory Conservatively allocates smaller KV cache Container overhead â‰ˆ KV cache reduction (almost 1:1) Smaller models suffer worse reduction ratios On the ~44 GiB native convergence:\nTensorRT-LLM likely has a KV cache ceiling (~44 GB) Small models hit the ceiling despite having more free memory Large models allocate near the ceiling despite tight budgets This is good engineering: prevents pathological cases Model size doesn\u0026rsquo;t predict KV cache availability What\u0026rsquo;s Next? This deep dive answered the \u0026ldquo;what\u0026rdquo; and \u0026ldquo;why\u0026rdquo; of the KV cache patterns. But there are still open questions:\nCan we configure TensorRT-LLM\u0026rsquo;s KV cache ceiling? Can Docker be patched to handle unified memory correctly? Do other unified memory systems (AMD MI300X) show the same pattern? What\u0026rsquo;s the optimal KV cache size for different workloads? These are questions for Phase 2 of the investigation. For now, the message is clear:\nIf you\u0026rsquo;re running large language models on Grace Hopper in production, understand your KV cache allocation. It\u0026rsquo;s the difference between maximal throughput and leaving half your serving capacity on the table.\nPrevious: â† Part 5: What I Learned (And What\u0026rsquo;s Next)\nGitHub Repo: benchmark-spark Interactive Charts: Results Dashboard\nGot questions or observations? Open an issue or discussion on the GitHub repo!\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/06-kv-cache-deep-dive/","summary":"\u003ch2 id=\"the-pattern-that-demands-explanation\"\u003eThe Pattern That Demands Explanation\u003c/h2\u003e\n\u003cp\u003eIn \u003ca href=\"../04-the-data\"\u003ePart 4\u003c/a\u003e, I showed you 60 benchmark runs that revealed a consistent pattern. But one table in particular kept me up at night:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eModel\u003c/th\u003e\n          \u003cth\u003eNative KV\u003c/th\u003e\n          \u003cth\u003eContainer KV\u003c/th\u003e\n          \u003cth\u003eReduction Factor\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDeepSeek-7B\u003c/td\u003e\n          \u003ctd\u003e44.31 GiB\u003c/td\u003e\n          \u003ctd\u003e16.57 GiB\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e2.7x less\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGPT-OSS-120B\u003c/td\u003e\n          \u003ctd\u003e43.19 GiB\u003c/td\u003e\n          \u003ctd\u003e23.65 GiB\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e1.8x less\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eQwen2.5-72B\u003c/td\u003e\n          \u003ctd\u003e44.71 GiB\u003c/td\u003e\n          \u003ctd\u003e26.72 GiB\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003e1.7x less\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eTwo questions immediately jump out:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eWhy do containers consistently allocate ~2x less KV cache?\u003c/strong\u003e (The main mystery)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWhy do all native runs converge to ~44 GiB?\u003c/strong\u003e (The secondary puzzle)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eLet\u0026rsquo;s answer both.\u003c/p\u003e","title":"KV Cache Deep Dive: The 2x Reduction Mystery"},{"content":"The Journey So Far Let\u0026rsquo;s recap this wild ride:\nThe Problem: YouTube reviewers blamed NVIDIA hardware for being slow The Investigation: I found 20-30GB memory overhead in Docker containers The Environment Setup: MPI and chroot configuration nightmare The Revelation: Docker\u0026rsquo;s cgroups double-count unified memory The Data: 60 runs confirmed the pattern consistently Now, what do I actually do with this knowledge?\nKey Finding: Don\u0026rsquo;t Blame the Hardware The most important lesson from this entire investigation:\nHardware isn\u0026rsquo;t the problem when you haven\u0026rsquo;t understood the software stack.\nThose YouTube reviews that said \u0026ldquo;DGX Spark is slow\u0026rdquo; or \u0026ldquo;Grace Hopper is disappointing\u0026rdquo;? They were wrong. Not because the numbers were wrong, but because they stopped at the numbers.\nThe hardware is fine. The software assumptions are outdated.\nDocker\u0026rsquo;s cgroups were designed in an era of discrete GPUs where:\nCPU RAM and GPU VRAM are separate Memory spaces don\u0026rsquo;t overlap No double-counting is possible Grace Hopper introduced unified memory:\nOne coherent memory pool Both processors access the same RAM Elegant\u0026hellip; but Docker doesn\u0026rsquo;t understand it yet The lesson: Dig deeper. Understand the full stack. Don\u0026rsquo;t just blame the hardware.\nPractical Recommendations Based on my findings, here\u0026rsquo;s what I recommend:\nFor Large Models on Grace Hopper (\u0026gt; 10B params): âœ… Use Native/Chroot Execution\nWhy:\n20-30 GB memory savings 1.7-2.7x more KV cache No performance penalty Better resource utilization Trade-off: Less isolation, more setup complexity\nFor Small Models (\u0026lt; 10B params): ğŸ¤” Docker is Acceptable\nIf 30GB overhead is acceptable for your use case:\nEasier deployment and management Better isolation for multi-tenancy Standard container tooling Simpler CI/CD integration For Discrete GPU Systems (H100, A100): âš ï¸ This Finding is Grace Hopper Specific\nTraditional discrete GPU systems should NOT exhibit this pattern because:\nGPU VRAM is outside Docker\u0026rsquo;s cgroups No double-counting possible Standard container best practices apply The KV Cache Mystery (Phase 2) Here\u0026rsquo;s what really caught my attention: The relationship between container overhead and KV cache.\nLook at the pattern across all models:\nModel Native Total Container Total Overhead Native KV Container KV KV Reduction DeepSeek-7B 70.47 GiB 101.30 GiB +30.83 GiB 44.31 GiB 16.57 GiB -27.74 GiB Qwen-72B 70.03 GiB 90.02 GiB +19.99 GiB 44.71 GiB 26.72 GiB -17.99 GiB GPT-OSS-120B 71.72 GiB 93.43 GiB +21.71 GiB 43.19 GiB 23.65 GiB -19.54 GiB The Real Question: Where is that 20-30 GB container overhead going? And why does it result in lower KV cache allocation?\nHypothesis: Docker\u0026rsquo;s cgroups are double-counting unified memory, making TensorRT-LLM think it has less available memory. The framework then conservatively allocates less KV cache to avoid OOM errors.\nNotice:\nAll three models use ~44 GiB KV cache in native mode (very similar!) Container overhead directly correlates with KV cache reduction The overhead isn\u0026rsquo;t going to computation - it\u0026rsquo;s just\u0026hellip; disappearing Phase 2 Goal: Figure out exactly where the container overhead is going and why it prevents proper KV cache allocation.\nPhase 2: Deep Dive into Container Memory Accounting I\u0026rsquo;m planning a comprehensive Phase 2 investigation to understand exactly where that overhead is going:\nThe Plan Profile memory allocation in real-time\nUse nvidia-smi dmon during container vs native runs Track CUDA memory allocation patterns Monitor cgroup memory accounting vs actual GPU usage Test Docker memory configurations\nDifferent cgroup versions (v1 vs v2) Various --gpus configurations Test with --privileged mode Try --ipc=host and other isolation tweaks Instrument TensorRT-LLM\nAdd logging to see how much memory it thinks is available Track KV cache allocation decisions Compare memory queries between environments Compare with discrete GPUs\nRun same tests on H100/A100 system Confirm this is Grace Hopper unified memory specific Establish baseline for normal Docker behavior Key Questions to Answer Where is the 20-30 GB going? Is it actually allocated, or just counted differently? Why does TensorRT-LLM allocate less KV cache? What signal is it reading? Can Docker be configured to handle unified memory? Are there flags/configs we\u0026rsquo;re missing? Is this NVIDIA Container Toolkit specific? Would native containerd or podman behave differently? Expected Outcomes Pinpoint the exact mechanism causing double-counting Determine if there\u0026rsquo;s a Docker configuration fix Document whether this affects other unified memory systems (AMD MI300X, future Intel solutions) Provide concrete recommendations for Grace Hopper containerization Share Your Findings If you\u0026rsquo;re running Grace Hopper systems (or other unified memory architectures), I\u0026rsquo;d love to hear from you:\nAre you seeing similar patterns? Have you found workarounds? Do you have additional data to share? GitHub Repo: benchmark-spark\nOpen an issue or submit a PR with your findings!\nResources All the code, data, and analysis are open source:\nğŸ“Š Interactive Results: brandonrc.github.io/benchmark-spark ğŸ“„ Full Analysis: ANALYSIS.md ğŸ”§ Benchmark Scripts: scripts/ ğŸ“¦ Raw Data: results/comprehensive/ Final Thoughts This investigation reinforced something fundamental:\nModern AI infrastructure is a stack:\nHardware (Grace Hopper) Kernel (Linux cgroups) Drivers (NVIDIA, CUDA) Runtime (Docker, containerd) Software (TensorRT-LLM, PyTorch) Applications (Your LLM workload) A problem at any layer can look like a problem at any other layer.\nWhen something seems slow or inefficient, resist the urge to blame the most visible component (usually the hardware or the framework). Instead:\nMeasure everything - Get real data Isolate variables - Test different configurations Understand the stack - Know what each layer does Share findings - Help the community The YouTubers who blamed NVIDIA weren\u0026rsquo;t doing engineering. They were doing performance theater.\nI did engineering. And I found the real answer.\nWhat\u0026rsquo;s Your Experience? Have you encountered similar issues? Different findings? Better solutions?\nComment on GitHub Discussions Share your data Help me build Phase 2 Together, you and I can make GPU computing better for everyone - by actually understanding it instead of just pointing fingers.\nPrevious: â† Part 4: The Data - 60 Runs Don\u0026rsquo;t Lie Next: Part 6: KV Cache Deep Dive - The 2x Reduction Mystery â†’\nGitHub Repo: benchmark-spark Phase 2 Tracking: GitHub Issues\nThanks for following along! ğŸš€\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/05-what-we-learned/","summary":"\u003ch2 id=\"the-journey-so-far\"\u003eThe Journey So Far\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s recap this wild ride:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eThe Problem\u003c/strong\u003e: YouTube reviewers blamed NVIDIA hardware for being slow\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe Investigation\u003c/strong\u003e: I found 20-30GB memory overhead in Docker containers\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe Environment Setup\u003c/strong\u003e: MPI and chroot configuration nightmare\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe Revelation\u003c/strong\u003e: Docker\u0026rsquo;s cgroups double-count unified memory\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe Data\u003c/strong\u003e: 60 runs confirmed the pattern consistently\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNow, what do I actually \u003cstrong\u003edo\u003c/strong\u003e with this knowledge?\u003c/p\u003e\n\u003ch2 id=\"key-finding-dont-blame-the-hardware\"\u003eKey Finding: Don\u0026rsquo;t Blame the Hardware\u003c/h2\u003e\n\u003cp\u003eThe most important lesson from this entire investigation:\u003c/p\u003e","title":"What I Learned (And What's Next)"},{"content":"The Comprehensive Test Plan Anecdotes are interesting. Single data points are suggestive. But 60 benchmark runs across multiple models? That\u0026rsquo;s science.\nHere\u0026rsquo;s what I did:\nTest Matrix 3 Models: DeepSeek-R1-Distill-Qwen-7B (7 billion parameters) Qwen2.5-72B-Instruct (72 billion parameters) GPT-OSS-120B (120 billion parameters, MXFP4 quantized) 2 Environments: Native (chroot) vs Container (Docker) 10 Iterations per model per environment Total: 60 benchmark runs Methodology Framework: TensorRT-LLM (trtllm-bench CLI) Workload: 50 requests, 128 output tokens per request Cooldown: 5 minutes + GPU temp check (\u0026lt; 45Â°C) between runs Duration: ~14 hours total Metrics: Peak memory, KV cache, throughput, latency, temperature DeepSeek-7B Results My smallest model, but the most shocking results:\nMetric Native Container Difference Peak Memory 70.47 GiB 101.30 GiB +30.83 GiB (44% overhead!) KV Cache 44.31 GiB 16.57 GiB -27.74 GiB (63% less!) Throughput 119.79 tok/s 119.40 tok/s 0.3% difference Std Dev (Ïƒ) 0.55 0.16 Very stable Analysis: The 7B model shows the largest overhead at 30.8GB. Container KV cache is reduced to only 37% of native. That\u0026rsquo;s massive.\nGPT-OSS-120B Results The largest model (120B params, though MoE means 5.1B active):\nMetric Native Container Difference Peak Memory 71.72 GiB 93.43 GiB +21.71 GiB (30% overhead) KV Cache 43.19 GiB 23.65 GiB -19.54 GiB (45% less) Throughput 120.26 tok/s 120.41 tok/s -0.1% difference Std Dev (Ïƒ) 0.32 0.54 Very stable Analysis: Despite being the largest model, GPT-OSS shows moderate overhead due to MXFP4 quantization. Native still has 1.8x more KV cache.\nQwen2.5-72B Results The 72B parameter model - the sweet spot:\nMetric Native Container Difference Peak Memory 70.03 GiB 90.02 GiB +19.99 GiB (29% overhead) KV Cache 44.71 GiB 26.72 GiB -17.99 GiB (40% less) Throughput 119.33 tok/s 119.51 tok/s -0.2% difference Std Dev (Ïƒ) 0.28 0.20 Very stable Analysis: Qwen shows the most efficient memory usage of all models. Native mode has the highest KV cache at 44.71 GiB - even more than the 7B model!\nThe Pattern: Container Overhead Scales Look at the overhead across models:\nModel Size Overhead Pattern DeepSeek 7B +30.83 GiB (44%) Highest % Qwen 72B +19.99 GiB (29%) Middle GPT-OSS 120B +21.71 GiB (30%) Middle Insight: Overhead appears proportional to base memory usage, not model size. This suggests Docker\u0026rsquo;s cgroup accounting scales with allocation size, confirming our unified memory double-counting theory.\nPerformance Parity: The Good News Across all 60 runs, performance was virtually identical:\nT - - - h r N C D o a o i u t n f g i t f h v a e p e i r u : n e t e n r c R : e a : n 1 1 g 1 1 \u0026lt; e 9 9 : . . 0 3 4 . 3 0 3 % - - ( 1 1 w 2 2 i 0 0 t . . h 2 5 i 6 1 n t t m o o a k k r e e g n n i s s n / / s s o e e f c c e r r o r ) Standard deviations were incredibly low (Ïƒ \u0026lt; 0.6 tokens/sec), showing:\nExcellent thermal management Consistent GPU performance No thermal throttling The KV Cache Revelation This is the real story. Across all models:\nModel Native KV Container KV Ratio DeepSeek 44.31 GiB 16.57 GiB 2.7x more GPT-OSS 43.19 GiB 23.65 GiB 1.8x more Qwen 44.71 GiB 26.72 GiB 1.7x more Native mode provides 1.7-2.7x more KV cache across the board. This is huge for:\nLonger context windows Higher batch sizes Better concurrent request handling Overall throughput scaling Temperature Analysis Interestingly, containers ran slightly cooler:\nEnvironment Avg End Temp Range Native 60.6Â°C 59-61Â°C Container 58.6Â°C 57-59Â°C Why? Container overhead means more time in cooldown between runs. Actual compute time is the same, but total wall time is longer due to additional memory management.\nData Quality and Reproducibility All 60 runs completed successfully with:\nâœ… 0 failures - Every benchmark completed âœ… Consistent results - Very low standard deviations âœ… Proper thermal management - No throttling âœ… Comprehensive logging - Full metadata saved Raw data available: GitHub - benchmark-spark/results\nInteractive Charts Want to explore the data visually? Check out our interactive results page with:\nPeak memory comparisons KV cache allocation charts Throughput performance graphs What This Proves After 60 runs across 3 models, the findings are clear:\nContainer overhead is real: 20-30 GB consistently KV cache reduction is significant: 1.7-2.7x less in containers Performance is identical: No speed penalty for native execution Pattern is consistent: Happens across all model sizes Root cause confirmed: Unified memory + cgroup double-counting This isn\u0026rsquo;t a fluke. This isn\u0026rsquo;t a configuration error. This is a fundamental architectural mismatch.\nNext up: What we learned, practical recommendations, and what we\u0026rsquo;re investigating next (spoiler: that KV cache scaling is fascinating).\nPrevious: â† Part 3: The Unified Memory Revelation\nNext: Part 5: What We Learned (And What\u0026rsquo;s Next) â†’\nGitHub Repo: benchmark-spark\nInteractive Charts: Results Dashboard\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/04-the-data/","summary":"\u003ch2 id=\"the-comprehensive-test-plan\"\u003eThe Comprehensive Test Plan\u003c/h2\u003e\n\u003cp\u003eAnecdotes are interesting. Single data points are suggestive. But \u003cstrong\u003e60 benchmark runs\u003c/strong\u003e across multiple models? That\u0026rsquo;s science.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s what I did:\u003c/p\u003e\n\u003ch3 id=\"test-matrix\"\u003eTest Matrix\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e3 Models\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eDeepSeek-R1-Distill-Qwen-7B (7 billion parameters)\u003c/li\u003e\n\u003cli\u003eQwen2.5-72B-Instruct (72 billion parameters)\u003c/li\u003e\n\u003cli\u003eGPT-OSS-120B (120 billion parameters, MXFP4 quantized)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e2 Environments\u003c/strong\u003e: Native (chroot) vs Container (Docker)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e10 Iterations\u003c/strong\u003e per model per environment\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal\u003c/strong\u003e: 60 benchmark runs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"methodology\"\u003eMethodology\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFramework\u003c/strong\u003e: TensorRT-LLM (\u003ccode\u003etrtllm-bench\u003c/code\u003e CLI)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWorkload\u003c/strong\u003e: 50 requests, 128 output tokens per request\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCooldown\u003c/strong\u003e: 5 minutes + GPU temp check (\u0026lt; 45Â°C) between runs\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDuration\u003c/strong\u003e: ~14 hours total\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMetrics\u003c/strong\u003e: Peak memory, KV cache, throughput, latency, temperature\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"deepseek-7b-results\"\u003eDeepSeek-7B Results\u003c/h2\u003e\n\u003cp\u003eMy smallest model, but the most shocking results:\u003c/p\u003e","title":"The Data: 60 Runs Don't Lie"},{"content":"The Question That Started It All After getting both Docker and native environments working, I could finally run proper benchmarks. But I kept asking myself:\n\u0026ldquo;Where is the 26GB going?\u0026rdquo;\nIt wasn\u0026rsquo;t CPU overhead - containers don\u0026rsquo;t add 26GB of process memory.\nIt wasn\u0026rsquo;t the Docker daemon - that\u0026rsquo;s tiny.\nIt wasn\u0026rsquo;t duplicate libraries - bind mounts prevent that.\nSo\u0026hellip; where?\nTraditional GPU Systems (The Old Way) Let\u0026rsquo;s start with how most GPU systems work. Take an NVIDIA H100 or A100:\nâ”Œ â”‚ â”‚ â”‚ â”‚ â”” â”€ â”€ â”€ â”€ â”€ D 6 â”€ â”€ C D 4 â”€ â”€ P R - â”€ â”€ U 5 â”€ â”€ R 1 â”€ â”€ ( A 2 â”€ â”€ H M â”€ â”€ o G â”€ â”€ s B â”€ â”€ t â”€ â”€ ) â”€ â”€ â”€ â”€ â”€ â”€ â”€ â” â”‚ â”‚ â”‚ â”‚ â”˜ â—„ â”€ P â”€ C â”€ I â”€ e â–º â”Œ â”‚ â”‚ â”‚ â”‚ â”” â”€ â”€ â”€ â”€ â”€ â”€ â”€ G H 4 â”€ â”€ P B 0 â”€ â”€ U M - â”€ â”€ 8 â”€ â”€ ( ( 0 â”€ â”€ D V â”€ â”€ e R G â”€ â”€ v A B â”€ â”€ i M â”€ â”€ c ) â”€ â”€ e â”€ â”€ ) â”€ â”€ â”€ â” â”‚ â”‚ â”‚ â”‚ â”˜ Key points:\nCPU has its own RAM (DDR) GPU has its own VRAM (HBM) Data moves between them over PCIe bus They\u0026rsquo;re separate memory spaces When Docker runs on these systems:\nDocker\u0026rsquo;s cgroups manage CPU RAM only GPU VRAM is outside Docker\u0026rsquo;s control nvidia-docker just passes through GPU access No double-counting because they\u0026rsquo;re separate Grace Hopper: The Game Changer Now look at Grace Hopper (our DGX Spark):\nâ”Œ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”” â”€ â”€ â”€ â”€ â”€ â”Œ â”‚ â”‚ â”‚ â”‚ â”‚ â”” â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ G â”€ U â”Œ â”‚ â”” C â”€ â”€ â”€ r â”€ N â”€ â”€ o â”€ â”€ â”€ a â”€ I â”€ A â”€ h â”€ â”€ â”€ c â”€ F â”€ R â”€ e â”€ â”€ â”€ e â”€ I â”€ M â”€ r â”€ â”€ â”€ â”€ E â”€ â”€ e â”€ â”€ â”€ H â”€ D â”€ C â”€ n â”€ â”€ â”€ o â”€ â”€ P â”€ t â”€ â”€ â”€ p â”€ M â”€ U â”€ , â”€ â”€ â”€ p â”€ E â”€ â”€ â”€ â”€ â”€ e â”€ M â” â”‚ â”˜ C â”€ â”€ â”€ r â”€ O â—„ a â”€ â”€ â”€ â”€ R â”€ c â”€ â”€ â”€ S â”€ Y â”€ h â”€ â”€ â”€ y â”€ â–º e â”€ â”€ â”€ s â”€ ( â”Œ â”‚ â”” - â”€ â”€ â”€ t â”€ 1 â”€ â”€ C â”€ â”€ â”€ e â”€ 2 â”€ G â”€ o â”€ â”€ â”€ m â”€ 8 â”€ B â”€ h â”€ â”€ â”€ â”€ â”€ 1 â”€ e â”€ â”€ â”€ â”€ G â”€ 0 â”€ r â”€ â”€ â”€ â”€ B â”€ â”€ e â”€ â”€ â”€ â”€ ) â”€ G â”€ n â”€ â”€ â”€ â”€ â”€ P â”€ t â”€ â”€ â”€ â”€ â”€ U â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â” â”‚ â”˜ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â” â”‚ â”‚ â”‚ â”‚ â”‚ â”˜ â”€ â”€ â”€ â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”˜ Revolutionary differences:\nOne memory pool for both CPU and GPU No PCIe transfers - both access the same RAM Coherent at the hardware level The CPU \u0026ldquo;sees\u0026rdquo; GPU allocations and vice versa This is elegant! No more copying between CPU and GPU. It\u0026rsquo;s all one big shared memory space.\nThe Docker cgroup Problem Here\u0026rsquo;s where things go sideways.\nDocker uses Linux cgroups (control groups) to isolate and track container resources:\n1 2 3 # Docker creates a memory cgroup for each container /sys/fs/cgroup/docker/\u0026lt;container_id\u0026gt;/memory.max /sys/fs/cgroup/docker/\u0026lt;container_id\u0026gt;/memory.current On a traditional GPU system, cgroups see:\nCPU RAM: Managed by cgroup âœ“ GPU VRAM: Outside cgroup (invisible) âœ“ On Grace Hopper unified memory, cgroups see:\nThe entire 128GB pool as \u0026ldquo;system RAM\u0026rdquo; âœ— The Double-Counting Here\u0026rsquo;s what happens when you run a model in a Docker container on Grace Hopper:\nModel loads into unified memory (let\u0026rsquo;s say 70GB) CUDA driver records this as GPU allocation Docker\u0026rsquo;s cgroup sees 70GB of \u0026ldquo;container RAM\u0026rdquo; used TensorRT-LLM tries to allocate KV cache Docker thinks: \u0026ldquo;Container is using 70GB already, only X GB left\u0026rdquo; Reality: That 70GB is THE SAME MEMORY, just counted twice! Result: Docker reserves extra headroom because it thinks GPU memory is separate \u0026ldquo;container RAM\u0026rdquo;, even though it\u0026rsquo;s not.\nThe Evidence Let\u0026rsquo;s look at what we saw:\nNative (Chroot) T M L K o o e V t d a a e v c l l e a s c u p : h n e e i a f k a i l e u l d s o a c m g a e e t m : e o d r : y : 1 7 4 3 1 8 1 7 9 . . . . 3 3 2 6 1 3 6 4 G G G G B B B B ( 9 0 % o f a v a i l a b l e ) Container (Docker) T M L K o o e V t d a a e v c l l e a s c u p : h n e e i a f k a i l e u l d s o a c m g a e e t m : e o d r : y : 1 1 1 1 1 0 4 3 9 4 . . . . 7 3 6 9 2 1 4 2 G G G G B B B B ( 9 â† 0 % W h o a f t ? a ! v a i l a b l e ) Docker\u0026rsquo;s cgroup sees 104.92GB used, but the actual model only needs 78GB. The difference (26.6GB) is phantom overhead from Docker\u0026rsquo;s memory accounting trying to \u0026ldquo;reserve\u0026rdquo; space that\u0026rsquo;s already in use.\nWhy Isn\u0026rsquo;t This a Problem on Discrete GPUs? On H100/A100 systems, cgroups can\u0026rsquo;t even see GPU VRAM. It\u0026rsquo;s on a separate PCIe device. So there\u0026rsquo;s no double-counting:\nD - - - - U - - - - i n s c C T N i c C D C c g U o o f g U o r r r D t i r D c e e o A a e o A k a t u l v d u e t e p t : e p t r e r r M r ' s G t a 6 l e t a s P r c 4 a m r c U a k p o a k a v : c s + r c s c e k : âœ“ y k : c r s 8 s o h : G 0 ( : P u e P G a n a C U = r \" r t d P a S t i U V 1 c y n âœ— R 4 e s o g R A 4 t f : A M G H e M B o m t C s p h o o e p R a n n p e A t f l a r M u y r ) \" s s a : a e ( t ( m d e e 1 e ! . l 2 g y 8 1 . G 2 , ( B 8 e ) G 6 . B 4 g G . B , ) 8 0 G B ) The KV Cache Impact This double-counting has a massive effect on KV cache:\nA - - - v a N C D i a o i l t n f a i t f b v a e l e i r e : n e e n f r c o : e r : 3 1 K 7 3 2 V . . . 2 3 8 c 6 1 x a c G G M h B B O e R : E i n n a t i v e m o d e More KV cache means:\nLonger context windows Higher batch sizes More concurrent requests Better overall throughput scaling That 26GB overhead isn\u0026rsquo;t just wasted RAM - it\u0026rsquo;s stolen capacity for serving workloads.\nWhy Performance Is Still The Same You might wonder: \u0026ldquo;If Docker has less KV cache, why is throughput identical?\u0026rdquo;\nGood question! For our specific benchmark (50 requests, 128 output tokens):\nEven 13GB of KV cache was enough We weren\u0026rsquo;t hitting the cache limit Throughput was compute-bound, not memory-bound But in production with:\nLonger contexts (8k, 32k, 128k tokens) Higher batch sizes Many concurrent users That reduced KV cache would absolutely become a bottleneck.\nCan Docker Be Fixed? Maybe? Potential solutions:\nUpdate nvidia-container-toolkit for unified memory awareness Use --memory=unlimited to disable cgroup memory limits Special cgroup configuration for Grace Hopper Wait for Docker/kernel patches that understand unified memory But for now, the simplest solution: Use native execution for large models on Grace Hopper.\nThe Takeaway This isn\u0026rsquo;t Docker being \u0026ldquo;bad\u0026rdquo; or Grace Hopper being \u0026ldquo;broken.\u0026rdquo; It\u0026rsquo;s a mismatch between technology generations:\nDocker\u0026rsquo;s cgroups: Designed for discrete GPU era Grace Hopper: Next-gen unified memory architecture Result: Software assumptions don\u0026rsquo;t match hardware reality And that\u0026rsquo;s why you can\u0026rsquo;t just blame the hardware. The entire stack matters.\nIn the next post, I\u0026rsquo;ll show you the data: 60 comprehensive benchmark runs across 3 different models, proving this pattern holds consistently.\nPrevious: â† Part 2: MPI and Chroot Nightmare\nNext: Part 4: The Data - 60 Runs Don\u0026rsquo;t Lie â†’\nGitHub Repo: benchmark-spark\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/03-unified-memory-revelation/","summary":"\u003ch2 id=\"the-question-that-started-it-all\"\u003eThe Question That Started It All\u003c/h2\u003e\n\u003cp\u003eAfter getting both Docker and native environments working, I could finally run proper benchmarks. But I kept asking myself:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026ldquo;Where is the 26GB going?\u0026rdquo;\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIt wasn\u0026rsquo;t CPU overhead - containers don\u0026rsquo;t add 26GB of process memory.\u003cbr\u003e\nIt wasn\u0026rsquo;t the Docker daemon - that\u0026rsquo;s tiny.\u003cbr\u003e\nIt wasn\u0026rsquo;t duplicate libraries - bind mounts prevent that.\u003c/p\u003e\n\u003cp\u003eSo\u0026hellip; where?\u003c/p\u003e\n\u003ch2 id=\"traditional-gpu-systems-the-old-way\"\u003eTraditional GPU Systems (The Old Way)\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s start with how most GPU systems work. Take an NVIDIA H100 or A100:\u003c/p\u003e","title":"The Unified Memory Revelation: Why Docker Double-Counts"},{"content":"The Simple Plan (That Wasn\u0026rsquo;t Simple) After seeing the mysterious 26GB memory overhead in Docker, my plan was straightforward:\nExtract the container\u0026rsquo;s filesystem Run the same Python scripts natively Compare the results Done! Ha. Hahahaha. No.\nAttempt 1: Just Run It My first thought: \u0026ldquo;Let\u0026rsquo;s just run the Docker scripts on my system. How hard can it be?\u0026rdquo;\n1 2 python /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py \\ --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B What I got: Runtime nightmare.\nM I C o m U d p D u o A l r e t E N E r o r r t r o F o r o r : u : n n d l o E i r b C r m U o p D r i A : . - s c N o a o . p 4 a m 0 b o : l d e u c l a d e n e n v n o i a t c m e e o d p i e s ' n t d e s e n h t s a e o r c r e t r d e t d _ o l b l j m e ' c t f i l e Of course. The container has TensorRT-LLM, specific CUDA versions, MPI libraries, and about a million other dependencies that my host system doesn\u0026rsquo;t have (or has different versions of).\nOkay, fine. Let\u0026rsquo;s extract the container and use chroot.\nExtracting the Container Docker containers are just fancy tarballs with layers. To extract:\n1 2 3 4 5 6 7 8 # Export the container filesystem docker create --name temp nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev docker export temp \u0026gt; container.tar docker rm temp # Extract to a directory mkdir -p /home/khan/container-rootfs sudo tar -xf container.tar -C /home/khan/container-rootfs Now I have the entire container filesystem at /home/khan/container-rootfs/. Cool!\nThe MPI Library Hunt Begins Time to run something:\n1 sudo chroot /home/khan/container-rootfs python3 /workspace/benchmarks/trtllm_benchmark.py New error:\n[ M T P R I T - E L r L r M o ] r : [ I u ] n d M e P f I i n s e i d z e s : y m 1 b , o l r : a n u k c : s _ 0 c o n f i g _ d o c _ n o p What? Let me check which MPI it\u0026rsquo;s using:\n1 2 which mpirun # /usr/bin/mpirun â† System MPI! Ah. The chroot is finding the system\u0026rsquo;s MPI (installed at /usr/bin/mpirun) instead of the container\u0026rsquo;s MPI (at /opt/hpcx/ompi/bin/mpirun inside the rootfs).\nThe PATH Dance TensorRT-LLM uses HPC-X OpenMPI from NVIDIA. The container has it at /opt/hpcx/ompi/. But when I chroot, the PATH still points to system binaries first.\nSolution: Explicitly set PATH to prioritize container binaries.\n1 export PATH=\u0026#34;/opt/hpcx/ompi/bin:$PATH\u0026#34; Run again\u0026hellip; new error:\nm p i r u n : e r r o r w h i l e l o a d i n g s h a r e d l i b r a r i e s : l i b u c s . s o . 0 : c a n n o t o p e n s h a r e d o b j e c t f i l e The MPI binary is now correct, but it can\u0026rsquo;t find its libraries!\nThe LD_LIBRARY_PATH Saga The container\u0026rsquo;s MPI needs libraries from /opt/hpcx/ompi/lib/, but LD_LIBRARY_PATH doesn\u0026rsquo;t include it.\n1 export LD_LIBRARY_PATH=\u0026#34;/opt/hpcx/ompi/lib:$LD_LIBRARY_PATH\u0026#34; Run again\u0026hellip; DIFFERENT error:\ns y m b o l l o o k u p e r r o r : o p t / h p c x / o m p i / l i b / l i b u c c . s o . 1 : u n d e f i n e d s y m b o l : u c p _ w o r k e r _ p r o g r e s s Wait, what? Now it\u0026rsquo;s finding the HPC-X libraries but they\u0026rsquo;re conflicting with system libraries!\nThe Real Problem Here\u0026rsquo;s what was happening:\nThe system has OpenMPI installed (libmpi.so) The container has HPC-X OpenMPI (/opt/hpcx/ompi/lib/libmpi.so) TensorRT-LLM needs the HPC-X version But the dynamic linker was mixing system and container libraries The fix: Put container libraries at the FRONT of LD_LIBRARY_PATH:\n1 export LD_LIBRARY_PATH=\u0026#34;/opt/hpcx/ompi/lib:/usr/local/lib/python3.12/dist-packages/tensorrt_libs:$LD_LIBRARY_PATH\u0026#34; Finally, MPI works!\nThe CUDA Version Surprise Got MPI working. Started a benchmark. New error:\nR u n t i m e E r r o r : T r i t o n o n l y s u p p o r t s C U D A 1 0 . 0 o r h i g h e r , b u t g o t C U D A v e r s i o n : 1 3 . 0 Wait, CUDA 13.0? We\u0026rsquo;re using CUDA 12.9!\nTurns out: The system symlink /usr/local/cuda pointed to CUDA 13.0. The container needs CUDA 12.9.\nFix:\n1 2 export CUDA_HOME=\u0026#34;/usr/local/cuda-12.9\u0026#34; export PATH=\u0026#34;/usr/local/cuda-12.9/bin:$PATH\u0026#34; The Full Chroot Wrapper Script After all this pain, I created a proper chroot wrapper that handles everything:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash SCRIPT_DIR=\u0026#34;/home/khan/container-rootfs\u0026#34; # Mount necessary filesystems sudo mount -t proc /proc \u0026#34;${SCRIPT_DIR}/proc\u0026#34; sudo mount --rbind /sys \u0026#34;${SCRIPT_DIR}/sys\u0026#34; sudo mount --rbind /dev \u0026#34;${SCRIPT_DIR}/dev\u0026#34; sudo mount --bind /home \u0026#34;${SCRIPT_DIR}/home\u0026#34; sudo mount --bind /data \u0026#34;${SCRIPT_DIR}/data\u0026#34; # DNS resolution sudo cp /etc/resolv.conf \u0026#34;${SCRIPT_DIR}/etc/resolv.conf\u0026#34; # Run in chroot with proper environment sudo chroot \u0026#34;${SCRIPT_DIR}\u0026#34; /usr/bin/env -i \\ HOME=/root \\ PATH=\u0026#34;/opt/hpcx/ompi/bin:/usr/local/cuda-12.9/bin:/usr/local/bin:/usr/bin:/bin\u0026#34; \\ LD_LIBRARY_PATH=\u0026#34;/opt/hpcx/ompi/lib:/usr/local/lib/python3.12/dist-packages/tensorrt_libs:/usr/local/cuda-12.9/lib64\u0026#34; \\ CUDA_HOME=\u0026#34;/usr/local/cuda-12.9\u0026#34; \\ PYTHONPATH=\u0026#34;/usr/local/lib/python3.12/dist-packages\u0026#34; \\ \u0026#34;$@\u0026#34; # Cleanup: unmount everything sudo umount \u0026#34;${SCRIPT_DIR}/data\u0026#34; sudo umount \u0026#34;${SCRIPT_DIR}/home\u0026#34; sudo umount \u0026#34;${SCRIPT_DIR}/dev\u0026#34; sudo umount \u0026#34;${SCRIPT_DIR}/sys\u0026#34; sudo umount \u0026#34;${SCRIPT_DIR}/proc\u0026#34; Now I can run:\n1 ./run_in_rootfs.sh python3 /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py And it actually works.\nLessons Learned Library paths matter: System vs container libraries will bite you Environment is everything: PATH, LD_LIBRARY_PATH, CUDA_HOME all critical MPI is picky: HPC-X OpenMPI isn\u0026rsquo;t interchangeable with system OpenMPI Filesystem mounts: Need /proc, /sys, /dev, /home, and /data bind mounts DNS matters: Even forgot /etc/resolv.conf initially! The Hard Part Is Over\u0026hellip; Right? With a working chroot environment, I could finally start benchmarking. But getting here took hours of debugging library paths and runtime errors.\nSometimes I wonder if this is why people just accept Docker\u0026rsquo;s overhead - at least it works out of the box!\nBut now that we have both Docker and native environments working, we can actually compare them fairly. And that\u0026rsquo;s where things get interesting\u0026hellip;\nIn the next post: What Grace Hopper\u0026rsquo;s unified memory architecture actually means, and why Docker\u0026rsquo;s cgroups don\u0026rsquo;t understand it.\nPrevious: â† Part 1: The Mystery\nNext: Part 3: The Unified Memory Revelation â†’\nGitHub Repo: benchmark-spark\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/02-mpi-chroot-nightmare/","summary":"\u003ch2 id=\"the-simple-plan-that-wasnt-simple\"\u003eThe Simple Plan (That Wasn\u0026rsquo;t Simple)\u003c/h2\u003e\n\u003cp\u003eAfter seeing the mysterious 26GB memory overhead in Docker, my plan was straightforward:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExtract the container\u0026rsquo;s filesystem\u003c/li\u003e\n\u003cli\u003eRun the same Python scripts natively\u003c/li\u003e\n\u003cli\u003eCompare the results\u003c/li\u003e\n\u003cli\u003eDone!\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHa. Hahahaha. \u003cstrong\u003eNo.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"attempt-1-just-run-it\"\u003eAttempt 1: Just Run It\u003c/h2\u003e\n\u003cp\u003eMy first thought: \u0026ldquo;Let\u0026rsquo;s just run the Docker scripts on my system. How hard can it be?\u0026rdquo;\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eWhat I got: \u003cstrong\u003eRuntime nightmare\u003c/strong\u003e.\u003c/p\u003e","title":"Down the Rabbit Hole: The MPI and Chroot Nightmare"},{"content":"The YouTube Problem If you search for \u0026ldquo;DGX Spark performance\u0026rdquo; on YouTube, you\u0026rsquo;ll find plenty of videos with clickbait titles like \u0026ldquo;NVIDIA\u0026rsquo;s $X Machine is a DISAPPOINTMENT\u0026rdquo; or \u0026ldquo;Grace Hopper: Overhyped and Underdelivering.\u0026rdquo;\nAnd that really bothers me.\nNot because I\u0026rsquo;m an NVIDIA fanboy (I\u0026rsquo;m not), but because none of these reviewers provided a technical explanation of why performance wasn\u0026rsquo;t meeting expectations. They just pointed at benchmark numbers, said \u0026ldquo;slow,\u0026rdquo; and moved on. No investigation into kernel settings, driver versions, container configurations, or software stack optimization. Just\u0026hellip; blame the hardware.\nThat\u0026rsquo;s not how engineering works.\nMy Setup: The New Kid on the Block I got my hands on an NVIDIA DGX Spark - a genuinely interesting piece of hardware:\nCPU: ARM Cortex (X925 + A725), 20 cores total GPU: NVIDIA GB10 (Blackwell architecture, Grace Hopper design) Memory: 128GB unified (CPU and GPU share the same RAM pool) Architecture: aarch64 (ARM64) The unified memory part is key. Unlike traditional systems where you have separate CPU RAM and GPU VRAM that copy data back and forth over PCIe, Grace Hopper has one coherent memory space. Both processors can access the same 128GB directly.\nPretty elegant, right?\nPrior Art: Docker GPU Passthrough Overhead Before diving into our own investigation, I found this paper: Benchmarking GPU Passthrough Performance on Docker for AI Cloud System.\nThe authors tested Docker GPU passthrough on consumer hardware (RTX 3060) and found:\nNative execution: Faster (1.52s avg) Docker containers: Slower (2.55s avg), but higher GPU utilization This validated that Docker overhead exists. But that study used consumer GPUs with simple matrix multiplication. I wanted to understand:\nDoes this apply to enterprise DGX hardware? What about production LLM workloads, not just matmul? WHY does this overhead exist in my specific case? The Test I ran TensorRT-LLM inference benchmarks in two environments:\nDocker Container (NVIDIA\u0026rsquo;s official image):\n1 2 3 docker run --rm --gpus all --ipc=host --shm-size=60g \\ nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev \\ python benchmarks/trtllm_benchmark.py Native (Chroot): Same code, same libraries, but running directly on the host using chroot to access the container\u0026rsquo;s filesystem.\nThe Shocking Result Environment Peak Memory KV Cache Available Docker Container 104.92 GiB 13.31 GiB Native (Chroot) 78.31 GiB 37.26 GiB Wait\u0026hellip; what?\nThe container was using 26.6 GB MORE memory and had less than half the KV cache available for the model!\nBut\u0026hellip; Performance Was Identical Here\u0026rsquo;s what made this even more confusing:\nN C a o t n i t v a e i : n e r : 1 1 2 2 0 1 . . 4 6 4 0 t t o o k k e e n n s s / / s s e e c c Same throughput. Same latency. The container wasn\u0026rsquo;t slower, it was just using way more memory for no apparent reason.\nThis wasn\u0026rsquo;t a \u0026ldquo;slow hardware\u0026rdquo; problem. This was something deeper.\nWhy This Matters You might think \u0026ldquo;who cares about 26GB if performance is the same?\u0026rdquo; But:\nKV Cache: That 26GB could enable longer contexts or higher batch sizes Multi-tenancy: Less memory per container = fewer workloads per box Cost: In cloud deployments, you pay for memory Principle: When hardware isn\u0026rsquo;t the problem, blaming hardware is lazy The Investigation Begins Rather than accept this as \u0026ldquo;Docker is bloated\u0026rdquo; or \u0026ldquo;Grace Hopper is broken,\u0026rdquo; I decided to dig in:\nRun comprehensive benchmarks (60 runs across 3 different model sizes) Test multiple LLMs: 7B, 72B, and 120B parameter models Understand what Grace Hopper\u0026rsquo;s unified memory architecture really means Figure out exactly where that 26GB is going The journey involved:\nMPI library path hell Chroot filesystem nightmares A revelation about Linux cgroups and unified memory Some genuinely surprising discoveries about KV cache scaling Spoiler: I found the root cause. And it\u0026rsquo;s not what you think.\nIn the next post, I\u0026rsquo;ll walk through the pain of setting up a proper test environment. Because sometimes the hardest part of debugging isn\u0026rsquo;t finding the bug - it\u0026rsquo;s just getting to a clean test in the first place.\nNext: Part 2: Down the Rabbit Hole - MPI and Chroot â†’\nGitHub Repo: benchmark-spark\nReferences:\nBenchmarking GPU Passthrough Performance on Docker for AI Cloud System ","permalink":"https://brandonrc.github.io/dgx-spark-blog/posts/01-the-mystery/","summary":"\u003ch2 id=\"the-youtube-problem\"\u003eThe YouTube Problem\u003c/h2\u003e\n\u003cp\u003eIf you search for \u0026ldquo;DGX Spark performance\u0026rdquo; on YouTube, you\u0026rsquo;ll find plenty of videos with clickbait titles like \u0026ldquo;NVIDIA\u0026rsquo;s $X Machine is a DISAPPOINTMENT\u0026rdquo; or \u0026ldquo;Grace Hopper: Overhyped and Underdelivering.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eAnd that really bothers me.\u003c/p\u003e\n\u003cp\u003eNot because I\u0026rsquo;m an NVIDIA fanboy (I\u0026rsquo;m not), but because \u003cstrong\u003enone of these reviewers provided a technical explanation\u003c/strong\u003e of why performance wasn\u0026rsquo;t meeting expectations. They just pointed at benchmark numbers, said \u0026ldquo;slow,\u0026rdquo; and moved on. No investigation into kernel settings, driver versions, container configurations, or software stack optimization. Just\u0026hellip; blame the hardware.\u003c/p\u003e","title":"The Mystery: Don't Just Blame the Hardware"},{"content":"Who \u0026amp; Why I\u0026rsquo;m Brandon Geraci, and I was tired of seeing YouTube tech reviewers blame NVIDIA\u0026rsquo;s DGX Spark hardware for being \u0026ldquo;slow\u0026rdquo; or \u0026ldquo;disappointing\u0026rdquo; without providing any technical analysis.\nSo I decided to actually investigate what was going on.\nWhat We Found After 60 comprehensive benchmarks across 3 different LLM models, we discovered:\nDocker containers use 20-30 GB more memory than native execution on Grace Hopper KV cache is reduced by 40-63% in containers Performance is identical - same throughput, no speed penalty Root cause: Docker\u0026rsquo;s cgroups double-count unified memory This isn\u0026rsquo;t a hardware problem. It\u0026rsquo;s a software stack mismatch.\nThe Project GitHub Repository: benchmark-spark\nAll code, data, and analysis are open source. We ran:\n60 total benchmark runs 3 models: DeepSeek-7B, Qwen-72B, GPT-OSS-120B 2 environments: Native (chroot) vs Container (Docker) 10 iterations per configuration ~14 hours of comprehensive testing Phase 2: What\u0026rsquo;s Next We\u0026rsquo;re planning a follow-up investigation:\nFactory reset the DGX Spark Create 1:1 bare metal vs container configs Deep dive into KV cache scaling Test on discrete GPU systems for comparison Get Involved If you\u0026rsquo;re running Grace Hopper or other unified memory systems:\nShare your findings Open issues on GitHub Contribute data or analysis Let\u0026rsquo;s understand this together - with engineering, not speculation.\nContact: GitHub\nProject: benchmark-spark\nInteractive Results: Results Dashboard\n","permalink":"https://brandonrc.github.io/dgx-spark-blog/about/","summary":"\u003ch2 id=\"who--why\"\u003eWho \u0026amp; Why\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m Brandon Geraci, and I was tired of seeing YouTube tech reviewers blame NVIDIA\u0026rsquo;s DGX Spark hardware for being \u0026ldquo;slow\u0026rdquo; or \u0026ldquo;disappointing\u0026rdquo; without providing any technical analysis.\u003c/p\u003e\n\u003cp\u003eSo I decided to actually investigate what was going on.\u003c/p\u003e\n\u003ch2 id=\"what-we-found\"\u003eWhat We Found\u003c/h2\u003e\n\u003cp\u003eAfter 60 comprehensive benchmarks across 3 different LLM models, we discovered:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDocker containers use 20-30 GB more memory\u003c/strong\u003e than native execution on Grace Hopper\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKV cache is reduced by 40-63%\u003c/strong\u003e in containers\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePerformance is identical\u003c/strong\u003e - same throughput, no speed penalty\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRoot cause\u003c/strong\u003e: Docker\u0026rsquo;s cgroups double-count unified memory\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis isn\u0026rsquo;t a hardware problem. It\u0026rsquo;s a software stack mismatch.\u003c/p\u003e","title":"About This Investigation"}]